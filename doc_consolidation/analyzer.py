"""
Content Analyzer component for the Documentation Consolidation System.

This module implements the ContentAnalyzer class that discovers, analyzes,
and categorizes markdown files in the source directory.
"""

import re
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

try:
    from .interfaces import ContentAnalyzerInterface
    from .models import (
        FileAnalysis, ConsolidationGroup, Category, ContentType, 
        Priority, ContentMetadata, ConsolidationStrategy
    )
    from .config import ConsolidationConfig, CATEGORY_PATTERNS, PRIORITY_PATTERNS
except ImportError:
    from interfaces import ContentAnalyzerInterface
    from models import (
        FileAnalysis, ConsolidationGroup, Category, ContentType, 
        Priority, ContentMetadata, ConsolidationStrategy
    )
    from config import ConsolidationConfig, CATEGORY_PATTERNS, PRIORITY_PATTERNS


class ContentAnalyzer(ContentAnalyzerInterface):
    """
    Analyzes and categorizes documentation files for consolidation.
    
    The ContentAnalyzer examines markdown files to determine their category,
    extract metadata, and identify consolidation opportunities based on
    content patterns and naming conventions.
    """
    
    def __init__(self, config: ConsolidationConfig):
        """Initialize the content analyzer with configuration."""
        self.config = config
        self.logger = logging.getLogger('doc_consolidation.analyzer')
        
        # Compile regex patterns for efficiency
        self._category_patterns = {}
        for category, patterns in CATEGORY_PATTERNS.items():
            self._category_patterns[category] = [
                re.compile(pattern, re.IGNORECASE) for pattern in patterns
            ]
        
        self._priority_patterns = {}
        for priority, patterns in PRIORITY_PATTERNS.items():
            self._priority_patterns[priority] = [
                re.compile(pattern, re.IGNORECASE) for pattern in patterns
            ]
    
    def discover_files(self, source_directory: str) -> List[Path]:
        """
        Discover all markdown files in the source directory recursively.
        
        Args:
            source_directory: Path to directory to scan for files
            
        Returns:
            List of Path objects for discovered markdown files
        """
        self.logger.info(f"Discovering files recursively in: {source_directory}")
        
        source_path = Path(source_directory)
        if not source_path.exists():
            self.logger.error(f"Source directory does not exist: {source_directory}")
            return []
        
        discovered_files = []
        
        # Get all files matching configured extensions recursively
        for extension in self.config.file_extensions:
            # Use recursive glob pattern to search subdirectories
            pattern = f"**/*{extension}"
            files = list(source_path.glob(pattern))
            discovered_files.extend(files)
        
        # Filter out excluded patterns and check file sizes
        filtered_files = []
        for file_path in discovered_files:
            if self._should_include_file(file_path):
                # Check file size and extract metadata
                try:
                    file_stats = file_path.stat()
                    file_size_mb = file_stats.st_size / (1024 * 1024)
                    
                    if file_size_mb <= self.config.max_file_size_mb:
                        filtered_files.append(file_path)
                        self.logger.debug(f"Discovered file: {file_path} "
                                        f"(size: {file_size_mb:.2f}MB, "
                                        f"modified: {datetime.fromtimestamp(file_stats.st_mtime)})")
                    else:
                        self.logger.warning(f"Skipping large file: {file_path} ({file_size_mb:.1f}MB)")
                        
                except OSError as e:
                    self.logger.warning(f"Could not check file size for {file_path}: {e}")
        
        self.logger.info(f"Discovered {len(filtered_files)} files for processing")
        return filtered_files
    
    def analyze_file(self, filepath: Path) -> FileAnalysis:
        """
        Analyze a single file to determine its category and metadata.
        
        Args:
            filepath: Path to the file to analyze
            
        Returns:
            FileAnalysis object containing categorization and metadata
        """
        self.logger.debug(f"Analyzing file: {filepath}")
        
        try:
            # Extract file system metadata first
            file_stats = filepath.stat()
            
            # Read file content with encoding detection
            content = self._read_file_content(filepath)
            
            # Extract metadata from content
            metadata = self.extract_content_metadata(content)
            
            # Add file system metadata to content metadata
            metadata.last_modified = datetime.fromtimestamp(file_stats.st_mtime)
            
            # Try to get creation date (platform dependent)
            try:
                # On Windows, st_ctime is creation time
                # On Unix, st_ctime is change time, st_birthtime is creation time (if available)
                if hasattr(file_stats, 'st_birthtime'):
                    metadata.creation_date = datetime.fromtimestamp(file_stats.st_birthtime)
                else:
                    # Fallback to modification time or content-extracted date
                    if not metadata.creation_date:
                        metadata.creation_date = metadata.last_modified
            except (OSError, AttributeError):
                # If we can't get creation date, use modification date as fallback
                if not metadata.creation_date:
                    metadata.creation_date = metadata.last_modified
            
            # Classify by filename pattern
            category, confidence = self.classify_by_pattern(filepath.name)
            
            # Enhance classification with content analysis if enabled
            if self.config.enable_content_analysis:
                content_category, content_confidence = self._classify_by_content(content)
                if content_confidence > confidence:
                    category = content_category
                    confidence = content_confidence
            
            # Determine content type
            content_type = self._determine_content_type(filepath.name, content, category)
            
            # Determine preservation priority
            priority = self._determine_priority(filepath.name, category)
            
            # Create analysis result
            analysis = FileAnalysis(
                filepath=filepath,
                filename=filepath.name,
                category=category,
                content_type=content_type,
                metadata=metadata,
                preservation_priority=priority,
                confidence_score=confidence
            )
            
            # Add processing notes
            if confidence < self.config.min_confidence_score:
                analysis.processing_notes.append(
                    f"Low confidence classification ({confidence:.2f})"
                )
            
            # Add file size information to processing notes
            file_size_mb = file_stats.st_size / (1024 * 1024)
            analysis.processing_notes.append(f"File size: {file_size_mb:.2f}MB")
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"Error analyzing file {filepath}: {e}")
            
            # Return minimal analysis for error cases
            return FileAnalysis(
                filepath=filepath,
                filename=filepath.name,
                category=Category.UNCATEGORIZED,
                content_type=ContentType.GENERAL_DOC,
                metadata=ContentMetadata(),
                preservation_priority=Priority.MEDIUM,
                confidence_score=0.0,
                processing_notes=[f"Analysis failed: {str(e)}"]
            )
    
    def classify_by_pattern(self, filename: str) -> Tuple[Category, float]:
        """
        Classify a file based on its filename pattern.
        Enhanced for Task 2.2: Implement pattern-based file classification
        Enhanced for Task 2.4: Handle edge cases for multi-category files
        
        Args:
            filename: Name of the file to classify
            
        Returns:
            Tuple of (Category, confidence_score)
        """
        best_category = Category.UNCATEGORIZED
        best_confidence = 0.0
        
        # Track all matches for multi-category handling
        category_matches = []
        
        for category, patterns in self._category_patterns.items():
            for pattern in patterns:
                if pattern.search(filename):
                    # Calculate confidence based on pattern specificity
                    confidence = self._calculate_pattern_confidence(pattern, filename)
                    category_matches.append((category, confidence, pattern.pattern))
                    
                    if confidence > best_confidence:
                        best_category = category
                        best_confidence = confidence
        
        # Enhanced pattern matching for specific task requirements
        enhanced_category, enhanced_confidence = self._enhanced_pattern_matching(filename)
        if enhanced_confidence > best_confidence:
            best_category = enhanced_category
            best_confidence = enhanced_confidence
            category_matches.append((enhanced_category, enhanced_confidence, "enhanced_pattern"))
        
        # Handle multi-category edge cases (Task 2.4)
        if len(category_matches) > 1:
            best_category, best_confidence = self._handle_multi_category_classification(
                filename, category_matches
            )
        
        # Log pattern matching details for debugging
        if category_matches:
            self.logger.debug(f"Pattern matches for '{filename}': {category_matches}")
        
        return best_category, best_confidence
    
    def _enhanced_pattern_matching(self, filename: str) -> Tuple[Category, float]:
        """
        Enhanced pattern matching for specific task 2.2 requirements.
        
        Args:
            filename: Name of the file to classify
            
        Returns:
            Tuple of (Category, confidence_score)
        """
        filename_upper = filename.upper()
        filename_lower = filename.lower()
        
        # Task 2.2: Enhanced testing documentation patterns (check first for priority)
        if self._is_testing_documentation(filename):
            return Category.TESTING_VALIDATION, 0.8
        
        # Task 2.2: Enhanced completion file patterns
        if self._is_completion_file(filename):
            return Category.IMPLEMENTATION_COMPLETION, 0.9
        
        # Task 2.2: Enhanced feature documentation patterns
        feature_category, feature_confidence = self._classify_feature_documentation(filename)
        if feature_confidence > 0.0:
            return feature_category, feature_confidence
        
        # Task 2.2: Enhanced setup and configuration patterns
        if self._is_setup_configuration_file(filename):
            return Category.SETUP_CONFIG, 0.85
        
        return Category.UNCATEGORIZED, 0.0
    
    def _is_completion_file(self, filename: str) -> bool:
        """
        Check if file matches completion patterns (Task 2.2).
        
        Args:
            filename: Name of the file to check
            
        Returns:
            True if file is a completion file
        """
        completion_patterns = [
            r'^TASK_.*_COMPLETE\.md$',      # TASK_*_COMPLETE.md
            r'^TASK_\d+.*_COMPLETE\.md$',   # TASK_1_COMPLETE.md, etc.
            r'^PHASE_.*_COMPLETE\.md$',     # PHASE_*_COMPLETE.md
            r'.*_COMPLETE\.md$',            # Any *_COMPLETE.md
            r'.*COMPLETE\.md$',             # Any *COMPLETE.md
            r'.*_SUMMARY\.md$',             # Any *_SUMMARY.md
            r'^Complete_Summary\.md$',      # Specific Complete_Summary.md
            r'^IMPLEMENTATION_COMPLETE\.md$',
            r'^MILESTONE_\d+_COMPLETE\.md$'
        ]
        
        for pattern_str in completion_patterns:
            if re.match(pattern_str, filename, re.IGNORECASE):
                return True
        
        return False
    
    def _classify_feature_documentation(self, filename: str) -> Tuple[Category, float]:
        """
        Classify feature documentation patterns (Task 2.2).
        
        Args:
            filename: Name of the file to classify
            
        Returns:
            Tuple of (Category, confidence_score)
        """
        # High-confidence feature patterns
        high_confidence_patterns = [
            (r'^PAYMENT_.*\.md$', 0.95),        # PAYMENT_* files
            (r'^TOURNAMENT_.*\.md$', 0.95),     # TOURNAMENT_* files
            (r'^AUTHENTICATION_.*\.md$', 0.9),  # AUTHENTICATION_* files
            (r'^AUTH_.*\.md$', 0.9),            # AUTH_* files
            (r'^NOTIFICATION_.*\.md$', 0.9),    # NOTIFICATION_* files
            (r'^DASHBOARD_.*\.md$', 0.9),       # DASHBOARD_* files
            (r'^USER_.*\.md$', 0.85),           # USER_* files
            (r'^TEAM_.*\.md$', 0.85),           # TEAM_* files
            (r'^COACHING_.*\.md$', 0.85),       # COACHING_* files
        ]
        
        for pattern_str, confidence in high_confidence_patterns:
            if re.match(pattern_str, filename, re.IGNORECASE):
                return Category.FEATURE_DOCS, confidence
        
        # Medium-confidence feature patterns
        medium_confidence_patterns = [
            (r'.*PAYMENT.*\.md$', 0.7),
            (r'.*TOURNAMENT.*\.md$', 0.7),
            (r'.*AUTH.*\.md$', 0.65),
            (r'.*NOTIFICATION.*\.md$', 0.7),
            (r'.*DASHBOARD.*\.md$', 0.7),
        ]
        
        for pattern_str, confidence in medium_confidence_patterns:
            if re.match(pattern_str, filename, re.IGNORECASE):
                return Category.FEATURE_DOCS, confidence
        
        return Category.UNCATEGORIZED, 0.0
    
    def _is_setup_configuration_file(self, filename: str) -> bool:
        """
        Check if file matches setup/configuration patterns (Task 2.2).
        
        Args:
            filename: Name of the file to check
            
        Returns:
            True if file is a setup/configuration file
        """
        setup_patterns = [
            r'.*_SETUP\.md$',           # Task 2.2: *_SETUP.md pattern
            r'^SETUP_.*\.md$',          # SETUP_* files
            r'.*SETUP.*\.md$',          # Any file with SETUP
            r'.*INSTALLATION.*\.md$',   # Installation files
            r'.*CONFIGURATION.*\.md$',  # Configuration files
            r'.*CONFIG.*\.md$',         # Config files
            r'.*ENVIRONMENT.*\.md$',    # Environment setup
            r'.*DEPLOYMENT.*\.md$',     # Deployment setup
            r'.*REQUIREMENTS.*\.md$',   # Requirements files
            r'^INSTALL.*\.md$',         # Install files
            r'.*INSTALL.*\.md$',        # Any install files
        ]
        
        for pattern_str in setup_patterns:
            if re.match(pattern_str, filename, re.IGNORECASE):
                return True
        
        return False
    
    def _is_testing_documentation(self, filename: str) -> bool:
        """
        Check if file matches testing documentation patterns (Task 2.2).
        
        Args:
            filename: Name of the file to check
            
        Returns:
            True if file is testing documentation
        """
        testing_patterns = [
            r'^test_.*\.md$',           # test_* files
            r'.*_test\.md$',            # *_test files
            r'.*_tests\.md$',           # *_tests files
            r'.*TEST.*\.md$',           # Any TEST files
            r'.*TESTING.*\.md$',        # Any TESTING files
            r'.*VALIDATION.*\.md$',     # Validation files
            r'.*COVERAGE.*\.md$',       # Coverage files
            r'.*REPORT.*\.md$',         # Test reports
            r'.*RESULTS.*\.md$',        # Test results
            r'.*BENCHMARK.*\.md$',      # Benchmark files
            r'.*PERFORMANCE.*\.md$',    # Performance test files
            r'^QA_.*\.md$',             # QA files
            r'.*QUALITY.*\.md$',        # Quality assurance files
        ]
        
        for pattern_str in testing_patterns:
            if re.match(pattern_str, filename, re.IGNORECASE):
                return True
        
        return False
    
    def extract_content_metadata(self, content: str) -> ContentMetadata:
        """
        Extract metadata from file content.
        Enhanced for Task 2.4: Implement content analysis and metadata extraction
        
        Args:
            content: Raw markdown content
            
        Returns:
            ContentMetadata object with extracted information
        """
        metadata = ContentMetadata()
        
        # Basic content statistics
        metadata.word_count = len(content.split())
        
        # Extract headings with enhanced parsing
        metadata.headings = self._extract_headings(content)
        
        # Count code blocks with enhanced detection
        metadata.code_blocks = self._count_code_blocks(content)
        
        # Check for tables with enhanced detection
        metadata.has_tables = self._detect_tables(content)
        
        # Check for images with enhanced detection
        metadata.has_images = self._detect_images(content)
        
        # Extract internal links with enhanced cross-reference detection
        metadata.internal_links = self._extract_internal_links(content)
        
        # Extract external links
        metadata.external_links = self._extract_external_links(content)
        
        # Extract key topics and themes with enhanced analysis
        metadata.key_topics = self._extract_key_topics_enhanced(content, metadata.headings)
        
        # Extract author attribution information
        metadata.author = self._extract_author_attribution(content)
        
        # Extract timestamp information with enhanced date parsing
        metadata.creation_date = self._extract_timestamp_information(content)
        
        return metadata
    
    def identify_consolidation_candidates(self, 
                                        analyses: List[FileAnalysis]) -> List[ConsolidationGroup]:
        """
        Identify groups of files that should be consolidated together.
        Enhanced for Task 4.1: Create consolidation group identification
        
        Args:
            analyses: List of FileAnalysis objects for all discovered files
            
        Returns:
            List of ConsolidationGroup objects representing consolidation opportunities
        """
        self.logger.info("Identifying consolidation candidates")
        
        consolidation_groups = []
        processed_files = set()
        
        # Group by category and content patterns
        category_groups = {}
        for analysis in analyses:
            if analysis.category not in category_groups:
                category_groups[analysis.category] = []
            category_groups[analysis.category].append(analysis)
        
        # Enhanced consolidation identification with multiple strategies
        
        # Strategy 1: Process completion summaries first (highest priority)
        completion_groups = self._identify_completion_summary_consolidation(analyses)
        consolidation_groups.extend(completion_groups)
        for group in completion_groups:
            processed_files.add(group.primary_file)
            processed_files.update(group.related_files)
        
        # Strategy 2: Process feature documentation groups
        feature_groups = self._identify_feature_consolidation(analyses, processed_files)
        consolidation_groups.extend(feature_groups)
        for group in feature_groups:
            processed_files.add(group.primary_file)
            processed_files.update(group.related_files)
        
        # Strategy 3: Process setup and configuration groups
        setup_groups = self._identify_setup_consolidation(analyses, processed_files)
        consolidation_groups.extend(setup_groups)
        for group in setup_groups:
            processed_files.add(group.primary_file)
            processed_files.update(group.related_files)
        
        # Strategy 4: Process testing documentation groups
        testing_groups = self._identify_testing_consolidation(analyses, processed_files)
        consolidation_groups.extend(testing_groups)
        for group in testing_groups:
            processed_files.add(group.primary_file)
            processed_files.update(group.related_files)
        
        # Strategy 5: Process remaining categories with generic grouping
        for category, category_analyses in category_groups.items():
            if category == Category.UNCATEGORIZED:
                continue
            
            # Filter out already processed files
            unprocessed_analyses = [
                analysis for analysis in category_analyses 
                if analysis.filename not in processed_files
            ]
            
            if len(unprocessed_analyses) > 1:
                groups = self._identify_category_groups(category, unprocessed_analyses)
                consolidation_groups.extend(groups)
                
                # Mark files as processed
                for group in groups:
                    processed_files.add(group.primary_file)
                    processed_files.update(group.related_files)
        
        # Strategy 6: Identify cross-category consolidation opportunities
        cross_category_groups = self._identify_cross_category_consolidation(analyses, processed_files)
        consolidation_groups.extend(cross_category_groups)
        
        # Validate and optimize consolidation groups
        consolidation_groups = self._optimize_consolidation_groups(consolidation_groups)
        
        self.logger.info(f"Identified {len(consolidation_groups)} consolidation groups using enhanced strategies")
        return consolidation_groups
    
    def validate_analysis(self, analysis: FileAnalysis) -> List[str]:
        """
        Validate an analysis result and return any issues found.
        
        Args:
            analysis: FileAnalysis object to validate
            
        Returns:
            List of validation error messages (empty if valid)
        """
        errors = []
        
        # Check if file exists
        if not analysis.filepath.exists():
            errors.append(f"File does not exist: {analysis.filepath}")
        
        # Check confidence score
        if not (0.0 <= analysis.confidence_score <= 1.0):
            errors.append(f"Invalid confidence score: {analysis.confidence_score}")
        
        # Check category assignment
        if analysis.category == Category.UNCATEGORIZED and analysis.confidence_score > 0.5:
            errors.append("High confidence but uncategorized file")
        
        # Check metadata consistency
        if analysis.metadata.word_count < 0:
            errors.append("Invalid word count in metadata")
        
        return errors
    
    def _should_include_file(self, file_path: Path) -> bool:
        """Check if a file should be included based on exclude patterns."""
        file_str = str(file_path)
        
        for pattern in self.config.exclude_patterns:
            if re.search(pattern.replace('*', '.*'), file_str):
                return False
        
        return True
    
    def _read_file_content(self, filepath: Path) -> str:
        """
        Read file content with robust encoding detection.
        
        Args:
            filepath: Path to the file to read
            
        Returns:
            File content as string
            
        Raises:
            Exception: If file cannot be read with any encoding
        """
        # List of encodings to try in order of preference
        encodings_to_try = [
            self.config.encoding,  # User-configured encoding first
            'utf-8',
            'utf-8-sig',  # UTF-8 with BOM
            'latin-1',
            'cp1252',     # Windows-1252
            'iso-8859-1'
        ]
        
        # Remove duplicates while preserving order
        seen = set()
        unique_encodings = []
        for enc in encodings_to_try:
            if enc not in seen:
                seen.add(enc)
                unique_encodings.append(enc)
        
        last_error = None
        
        for encoding in unique_encodings:
            try:
                with open(filepath, 'r', encoding=encoding) as f:
                    content = f.read()
                
                # Log if we had to use a different encoding than configured
                if encoding != self.config.encoding:
                    self.logger.warning(f"Used {encoding} encoding for {filepath} "
                                      f"(configured: {self.config.encoding})")
                else:
                    self.logger.debug(f"Successfully read {filepath} with {encoding} encoding")
                
                return content
                
            except UnicodeDecodeError as e:
                last_error = e
                self.logger.debug(f"Failed to read {filepath} with {encoding}: {e}")
                continue
            except Exception as e:
                # For other errors (file not found, permission, etc.), re-raise immediately
                self.logger.error(f"Error reading file {filepath}: {e}")
                raise
        
        # If all encodings failed, try binary read with error handling
        try:
            with open(filepath, 'rb') as f:
                raw_content = f.read()
            
            # Try to decode with UTF-8 and ignore errors
            content = raw_content.decode('utf-8', errors='ignore')
            
            self.logger.warning(f"Used fallback binary decoding for {filepath} "
                              f"(some characters may be lost)")
            return content
            
        except Exception as e:
            self.logger.error(f"Complete failure reading {filepath}: {e}")
            raise Exception(f"Could not read file {filepath} with any encoding. "
                          f"Last encoding error: {last_error}")
    
    
    def _classify_by_content(self, content: str) -> Tuple[Category, float]:
        """Classify file based on content analysis."""
        # Simple content-based classification
        content_lower = content.lower()
        
        # Look for specific keywords and patterns
        if any(word in content_lower for word in ['setup', 'install', 'configuration']):
            return Category.SETUP_CONFIG, 0.6
        
        if any(word in content_lower for word in ['test', 'testing', 'validation']):
            return Category.TESTING_VALIDATION, 0.6
        
        if any(word in content_lower for word in ['complete', 'finished', 'done']):
            return Category.IMPLEMENTATION_COMPLETION, 0.6
        
        if any(word in content_lower for word in ['payment', 'tournament', 'authentication']):
            return Category.FEATURE_DOCS, 0.6
        
        return Category.UNCATEGORIZED, 0.0
    
    def _determine_content_type(self, filename: str, content: str, category: Category) -> ContentType:
        """Determine the content type based on filename, content, and category."""
        filename_lower = filename.lower()
        content_lower = content.lower()
        
        if 'complete' in filename_lower or 'summary' in filename_lower:
            return ContentType.COMPLETION_SUMMARY
        
        if category == Category.SETUP_CONFIG:
            return ContentType.SETUP_PROCEDURE
        
        if category == Category.TESTING_VALIDATION:
            return ContentType.TEST_REPORT
        
        if category == Category.QUICK_REFERENCES:
            return ContentType.QUICK_REFERENCE
        
        if category == Category.INTEGRATION_GUIDES:
            return ContentType.INTEGRATION_GUIDE
        
        if category == Category.FEATURE_DOCS:
            return ContentType.FEATURE_GUIDE
        
        if category == Category.HISTORICAL_ARCHIVE:
            return ContentType.HISTORICAL_DOC
        
        return ContentType.GENERAL_DOC
    
    def _determine_priority(self, filename: str, category: Category) -> Priority:
        """Determine preservation priority based on filename and category."""
        for priority, patterns in self._priority_patterns.items():
            for pattern in patterns:
                if pattern.search(filename):
                    return priority
        
        # Default priorities by category
        category_priorities = {
            Category.SETUP_CONFIG: Priority.HIGH,
            Category.FEATURE_DOCS: Priority.HIGH,
            Category.INTEGRATION_GUIDES: Priority.HIGH,
            Category.QUICK_REFERENCES: Priority.MEDIUM,
            Category.IMPLEMENTATION_COMPLETION: Priority.MEDIUM,
            Category.TESTING_VALIDATION: Priority.LOW,
            Category.HISTORICAL_ARCHIVE: Priority.ARCHIVE,
            Category.UNCATEGORIZED: Priority.MEDIUM
        }
        
        return category_priorities.get(category, Priority.MEDIUM)
    
    def _calculate_pattern_confidence(self, pattern: re.Pattern, filename: str) -> float:
        """
        Calculate confidence score for a pattern match.
        Enhanced for Task 2.2 with more sophisticated confidence calculation.
        
        Args:
            pattern: Compiled regex pattern that matched
            filename: Filename that was matched
            
        Returns:
            Confidence score between 0.0 and 1.0
        """
        # Base confidence for any match
        confidence = 0.5
        
        pattern_str = pattern.pattern
        filename_upper = filename.upper()
        
        # Higher confidence for exact prefix matches
        if pattern_str.startswith('^'):
            confidence += 0.15
        
        # Higher confidence for exact suffix matches
        if pattern_str.endswith('$'):
            confidence += 0.1
        
        # Higher confidence for specific task patterns
        if 'TASK_.*_COMPLETE' in pattern_str:
            confidence += 0.3  # Very specific completion pattern
        elif '_COMPLETE' in pattern_str:
            confidence += 0.2  # General completion pattern
        
        # Higher confidence for specific feature patterns
        feature_prefixes = ['PAYMENT_', 'TOURNAMENT_', 'AUTH', 'NOTIFICATION_', 'DASHBOARD_']
        for prefix in feature_prefixes:
            if prefix in pattern_str and filename_upper.startswith(prefix):
                confidence += 0.25
                break
        
        # Higher confidence for setup patterns
        if '_SETUP' in pattern_str and filename_upper.endswith('_SETUP.MD'):
            confidence += 0.25
        
        # Increase confidence for more specific patterns (longer patterns)
        if len(pattern_str) > 25:
            confidence += 0.15
        elif len(pattern_str) > 15:
            confidence += 0.1
        
        # Decrease confidence for very generic patterns
        if pattern_str.count('.*') > 2:
            confidence -= 0.15
        elif pattern_str.count('.*') > 1:
            confidence -= 0.1
        
        # Bonus for patterns without wildcards (exact matches)
        if '.*' not in pattern_str and '*' not in pattern_str:
            confidence += 0.15
        
        # Penalty for very generic patterns like ".*[Tt]est.*"
        if pattern_str.startswith('.*') and pattern_str.endswith('.*'):
            confidence -= 0.1
        
        # Ensure confidence stays within bounds
        return min(max(confidence, 0.0), 1.0)
    
    def get_file_metadata(self, filepath: Path) -> Dict[str, any]:
        """
        Extract comprehensive file metadata including size, dates, and permissions.
        
        Args:
            filepath: Path to the file
            
        Returns:
            Dictionary containing file metadata
        """
        try:
            if not filepath.exists():
                return {}
            
            stat = filepath.stat()
            
            metadata = {
                'size_bytes': stat.st_size,
                'size_mb': stat.st_size / (1024 * 1024),
                'size_kb': stat.st_size / 1024,
                'modified_timestamp': stat.st_mtime,
                'modified_date': datetime.fromtimestamp(stat.st_mtime),
                'access_timestamp': stat.st_atime,
                'access_date': datetime.fromtimestamp(stat.st_atime),
                'change_timestamp': stat.st_ctime,
                'change_date': datetime.fromtimestamp(stat.st_ctime),
                'is_file': filepath.is_file(),
                'is_directory': filepath.is_dir(),
                'is_symlink': filepath.is_symlink(),
                'permissions': oct(stat.st_mode)[-3:],
                'file_extension': filepath.suffix.lower(),
                'stem_name': filepath.stem,
                'absolute_path': str(filepath.absolute()),
                'relative_path': str(filepath),
                'parent_directory': str(filepath.parent)
            }
            
            # Try to get creation date (platform-dependent)
            try:
                if hasattr(stat, 'st_birthtime'):
                    # macOS and some BSD systems
                    metadata['created_timestamp'] = stat.st_birthtime
                    metadata['created_date'] = datetime.fromtimestamp(stat.st_birthtime)
                elif hasattr(stat, 'st_crtime'):
                    # Some other systems
                    metadata['created_timestamp'] = stat.st_crtime
                    metadata['created_date'] = datetime.fromtimestamp(stat.st_crtime)
                else:
                    # Fallback: use change time as creation time approximation
                    metadata['created_timestamp'] = stat.st_ctime
                    metadata['created_date'] = datetime.fromtimestamp(stat.st_ctime)
            except (OSError, AttributeError):
                # If creation time is not available, use modification time
                metadata['created_timestamp'] = stat.st_mtime
                metadata['created_date'] = datetime.fromtimestamp(stat.st_mtime)
            
            # Calculate file age
            now = datetime.now()
            metadata['age_days'] = (now - metadata['modified_date']).days
            metadata['created_age_days'] = (now - metadata['created_date']).days
            
            self.logger.debug(f"Extracted metadata for {filepath}: "
                            f"{metadata['size_mb']:.2f}MB, "
                            f"modified {metadata['age_days']} days ago")
            
            return metadata
            
        except Exception as e:
            self.logger.error(f"Failed to extract metadata for {filepath}: {e}")
            return {}
    
    def validate_discovered_files(self, files: List[Path]) -> Dict[str, List[Path]]:
        """
        Validate discovered files and categorize any issues.
        
        Args:
            files: List of discovered file paths
            
        Returns:
            Dictionary with validation results categorized by issue type
        """
        validation_results = {
            'valid_files': [],
            'missing_files': [],
            'permission_errors': [],
            'large_files': [],
            'empty_files': [],
            'binary_files': [],
            'encoding_issues': []
        }
        
        for filepath in files:
            try:
                # Check if file exists
                if not filepath.exists():
                    validation_results['missing_files'].append(filepath)
                    continue
                
                # Check if it's actually a file
                if not filepath.is_file():
                    continue
                
                # Get file stats
                stat = filepath.stat()
                file_size_mb = stat.st_size / (1024 * 1024)
                
                # Check file size
                if file_size_mb > self.config.max_file_size_mb:
                    validation_results['large_files'].append(filepath)
                    continue
                
                # Check if file is empty
                if stat.st_size == 0:
                    validation_results['empty_files'].append(filepath)
                    continue
                
                # Try to read a small portion to check if it's text
                try:
                    with open(filepath, 'rb') as f:
                        sample = f.read(1024)  # Read first 1KB
                    
                    # Check if it looks like binary data
                    if b'\x00' in sample:
                        validation_results['binary_files'].append(filepath)
                        continue
                    
                    # Try to decode as text
                    try:
                        sample.decode('utf-8')
                    except UnicodeDecodeError:
                        # Try other encodings
                        encoding_found = False
                        for encoding in ['latin-1', 'cp1252', 'iso-8859-1']:
                            try:
                                sample.decode(encoding)
                                encoding_found = True
                                break
                            except UnicodeDecodeError:
                                continue
                        
                        if not encoding_found:
                            validation_results['encoding_issues'].append(filepath)
                            continue
                
                except PermissionError:
                    validation_results['permission_errors'].append(filepath)
                    continue
                
                # If we get here, the file passed all checks
                validation_results['valid_files'].append(filepath)
                
            except Exception as e:
                self.logger.warning(f"Validation error for {filepath}: {e}")
                # Add to encoding issues as a catch-all
                validation_results['encoding_issues'].append(filepath)
        
        # Log validation summary
        total_files = len(files)
        valid_files = len(validation_results['valid_files'])
        
        self.logger.info(f"File validation complete: {valid_files}/{total_files} files valid")
        
        for issue_type, issue_files in validation_results.items():
            if issue_type != 'valid_files' and issue_files:
                self.logger.warning(f"{issue_type}: {len(issue_files)} files")
        
        return validation_results
    
    def _extract_key_topics(self, content: str, headings: List[str]) -> List[str]:
        """Extract key topics from content and headings."""
        topics = set()
        
        # Add topics from headings
        for heading in headings:
            words = re.findall(r'\b\w+\b', heading.lower())
            topics.update(word for word in words if len(word) > 3)
        
        # Add topics from content (simple keyword extraction)
        important_words = re.findall(r'\b[A-Z][a-z]+\b', content)
        topics.update(word.lower() for word in important_words if len(word) > 4)
        
        return list(topics)[:10]  # Limit to top 10 topics
    
    def _extract_headings(self, content: str) -> List[str]:
        """
        Extract headings with enhanced parsing for Task 2.4.
        
        Args:
            content: Raw markdown content
            
        Returns:
            List of heading text with level information preserved
        """
        headings = []
        
        # Enhanced heading pattern that captures level and text
        heading_pattern = re.compile(r'^(#{1,6})\s+(.+)$', re.MULTILINE)
        
        for match in heading_pattern.finditer(content):
            level = len(match.group(1))  # Number of # characters
            text = match.group(2).strip()
            
            # Clean up heading text (remove markdown formatting)
            text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # Remove bold
            text = re.sub(r'\*(.*?)\*', r'\1', text)      # Remove italic
            text = re.sub(r'`(.*?)`', r'\1', text)        # Remove code
            text = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', text)  # Remove links, keep text
            
            headings.append(text)
        
        return headings
    
    def _count_code_blocks(self, content: str) -> int:
        """
        Count code blocks with enhanced detection for Task 2.4.
        
        Args:
            content: Raw markdown content
            
        Returns:
            Number of code blocks found
        """
        # Count fenced code blocks (```)
        fenced_pattern = re.compile(r'```[\s\S]*?```', re.MULTILINE)
        fenced_blocks = len(fenced_pattern.findall(content))
        
        # For now, only count fenced code blocks to maintain compatibility with existing tests
        # In the future, we could add indented code block detection as an enhancement
        
        return fenced_blocks
    
    def _detect_tables(self, content: str) -> bool:
        """
        Detect tables with enhanced parsing for Task 2.4.
        
        Args:
            content: Raw markdown content
            
        Returns:
            True if tables are detected
        """
        # Look for markdown table patterns
        table_patterns = [
            r'\|.*\|.*\n\|[-\s|:]+\|',  # Header with separator row
            r'^\|.*\|$.*\n^\|[-\s|:]+\|$',  # Multiline table pattern
            r'\|.*\|.*\|.*\|',  # Simple pipe-separated content (3+ columns)
        ]
        
        for pattern in table_patterns:
            if re.search(pattern, content, re.MULTILINE):
                return True
        
        return False
    
    def _detect_images(self, content: str) -> bool:
        """
        Detect images with enhanced parsing for Task 2.4.
        
        Args:
            content: Raw markdown content
            
        Returns:
            True if images are detected
        """
        # Enhanced image detection patterns
        image_patterns = [
            r'!\[.*?\]\(.*?\)',  # Standard markdown images
            r'!\[.*?\]\[.*?\]',  # Reference-style images
            r'<img\s+[^>]*src\s*=\s*["\'][^"\']*["\'][^>]*>',  # HTML img tags
            r'!\[[^\]]*\]\s*:\s*\S+',  # Reference definitions
        ]
        
        for pattern in image_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                return True
        
        return False
    
    def _extract_internal_links(self, content: str) -> List[str]:
        """
        Extract internal links with enhanced cross-reference detection for Task 2.4.
        
        Args:
            content: Raw markdown content
            
        Returns:
            List of internal link targets
        """
        internal_links = []
        
        # Standard markdown links (not starting with http/https/ftp)
        link_pattern = re.compile(r'\[([^\]]*)\]\(([^)]+)\)')
        for match in link_pattern.finditer(content):
            link_text = match.group(1)
            link_target = match.group(2).strip()
            
            # Skip external links
            if link_target.startswith(('http://', 'https://', 'ftp://', 'mailto:')):
                continue
            
            # Skip anchors without files
            if link_target.startswith('#'):
                continue
            
            internal_links.append(link_target)
        
        # Reference-style links
        ref_pattern = re.compile(r'\[([^\]]+)\]\s*:\s*([^\s]+)')
        for match in ref_pattern.finditer(content):
            link_target = match.group(2).strip()
            
            # Skip external links
            if link_target.startswith(('http://', 'https://', 'ftp://', 'mailto:')):
                continue
            
            internal_links.append(link_target)
        
        # Cross-references in text (e.g., "see file.md", "refer to doc.md")
        cross_ref_patterns = [
            r'(?:see|refer to|check|view|read)\s+([a-zA-Z0-9_-]+\.md)',
            r'(?:documented in|described in|found in)\s+([a-zA-Z0-9_-]+\.md)',
            r'(?:file|document):\s*([a-zA-Z0-9_-]+\.md)',
        ]
        
        for pattern in cross_ref_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            internal_links.extend(matches)
        
        # Remove duplicates while preserving order
        seen = set()
        unique_links = []
        for link in internal_links:
            if link not in seen:
                seen.add(link)
                unique_links.append(link)
        
        return unique_links
    
    def _extract_external_links(self, content: str) -> List[str]:
        """
        Extract external links with enhanced detection for Task 2.4.
        
        Args:
            content: Raw markdown content
            
        Returns:
            List of external link URLs
        """
        external_links = []
        
        # Standard markdown links
        link_pattern = re.compile(r'\[([^\]]*)\]\(([^)]+)\)')
        for match in link_pattern.finditer(content):
            link_target = match.group(2).strip()
            
            # Only include external links
            if link_target.startswith(('http://', 'https://', 'ftp://')):
                external_links.append(link_target)
        
        # Reference-style links
        ref_pattern = re.compile(r'\[([^\]]+)\]\s*:\s*([^\s]+)')
        for match in ref_pattern.finditer(content):
            link_target = match.group(2).strip()
            
            # Only include external links
            if link_target.startswith(('http://', 'https://', 'ftp://')):
                external_links.append(link_target)
        
        # Plain URLs in text (but be more careful about boundaries)
        url_pattern = re.compile(r'(?<!\]\()https?://[^\s<>"{}|\\^`\[\]()]+(?<![.,;:!?)])')
        plain_urls = url_pattern.findall(content)
        external_links.extend(plain_urls)
        
        # Remove duplicates while preserving order
        seen = set()
        unique_links = []
        for link in external_links:
            if link not in seen:
                seen.add(link)
                unique_links.append(link)
        
        return unique_links
    
    def _extract_key_topics_enhanced(self, content: str, headings: List[str]) -> List[str]:
        """
        Extract key topics and themes with enhanced analysis for Task 2.4.
        
        Args:
            content: Raw markdown content
            headings: List of extracted headings
            
        Returns:
            List of key topics and themes
        """
        topics = set()
        
        # Extract topics from headings with better processing
        for heading in headings:
            # Split on common separators and extract meaningful words
            words = re.findall(r'\b[a-zA-Z]{3,}\b', heading.lower())
            
            # Filter out common stop words
            stop_words = {'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day', 'get', 'has', 'him', 'his', 'how', 'its', 'may', 'new', 'now', 'old', 'see', 'two', 'who', 'boy', 'did', 'she', 'use', 'way', 'will', 'with'}
            
            meaningful_words = [word for word in words if word not in stop_words and len(word) > 3]
            topics.update(meaningful_words)
        
        # Extract technical terms and proper nouns from content
        technical_patterns = [
            r'\b[A-Z][a-zA-Z]*[A-Z][a-zA-Z]*\b',  # CamelCase terms
            r'\b[A-Z]{2,}\b',  # Acronyms
            r'\b[a-z]+_[a-z]+\b',  # snake_case terms
            r'\b[a-z]+-[a-z]+\b',  # kebab-case terms
        ]
        
        for pattern in technical_patterns:
            matches = re.findall(pattern, content)
            topics.update(match.lower() for match in matches if len(match) > 3)
        
        # Extract domain-specific terms
        domain_terms = [
            'authentication', 'authorization', 'payment', 'tournament', 'user', 'admin',
            'dashboard', 'notification', 'api', 'database', 'configuration', 'setup',
            'installation', 'deployment', 'testing', 'validation', 'integration',
            'security', 'performance', 'monitoring', 'logging', 'backup', 'migration'
        ]
        
        content_lower = content.lower()
        for term in domain_terms:
            if term in content_lower:
                topics.add(term)
        
        # Extract topics from code blocks and file references
        code_block_pattern = re.compile(r'```[\w]*\n([\s\S]*?)```')
        for match in code_block_pattern.finditer(content):
            code_content = match.group(1)
            # Extract class names, function names, etc.
            code_terms = re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]{2,}\b', code_content)
            topics.update(term.lower() for term in code_terms[:5])  # Limit to avoid noise
        
        # Prioritize topics by frequency and importance
        topic_scores = {}
        for topic in topics:
            # Count occurrences in content
            count = content_lower.count(topic)
            
            # Boost score for topics in headings
            heading_boost = sum(1 for heading in headings if topic in heading.lower()) * 2
            
            # Boost score for domain-specific terms
            domain_boost = 1 if topic in [t.lower() for t in domain_terms] else 0
            
            topic_scores[topic] = count + heading_boost + domain_boost
        
        # Return top topics sorted by score
        sorted_topics = sorted(topic_scores.items(), key=lambda x: x[1], reverse=True)
        return [topic for topic, score in sorted_topics[:15]]  # Top 15 topics
    
    def _extract_author_attribution(self, content: str) -> Optional[str]:
        """
        Extract author attribution information for Task 2.4.
        
        Args:
            content: Raw markdown content
            
        Returns:
            Author name if found, None otherwise
        """
        # Common author patterns in documentation
        author_patterns = [
            r'(?:author|by|created by|written by):\s*([a-zA-Z\s.]+)',
            r'(?:author|by|created by|written by)\s+([a-zA-Z\s.]+)',
            r'@author\s+([a-zA-Z\s.]+)',
            r'<!--\s*author:\s*([a-zA-Z\s.]+)\s*-->',
            r'^\s*\*\*author\*\*:\s*([a-zA-Z\s.]+)',
            r'^\s*author:\s*([a-zA-Z\s.]+)',
        ]
        
        for pattern in author_patterns:
            match = re.search(pattern, content, re.IGNORECASE | re.MULTILINE)
            if match:
                author = match.group(1).strip()
                # Clean up author name
                author = re.sub(r'\s+', ' ', author)  # Normalize whitespace
                author = author.strip('.,;')  # Remove trailing punctuation
                
                # Validate author name - should contain at least one letter and be reasonable length
                if (len(author) > 2 and len(author) < 50 and 
                    re.search(r'[a-zA-Z]', author) and
                    not re.search(r'\b(?:here|info|information|none|null|empty)\b', author, re.IGNORECASE)):
                    return author
        
        # Look for email signatures or commit-style attributions
        email_patterns = [
            r'([a-zA-Z\s.]+)\s*<[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}>',
            r'Signed-off-by:\s*([a-zA-Z\s.]+)',
            r'Committed by:\s*([a-zA-Z\s.]+)',
        ]
        
        for pattern in email_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                author = match.group(1).strip()
                if (len(author) > 2 and len(author) < 50 and
                    not re.search(r'\b(?:here|info|information|none|null|empty)\b', author, re.IGNORECASE)):
                    return author
        
        return None
    
    def _extract_timestamp_information(self, content: str) -> Optional[datetime]:
        """
        Extract timestamp information with enhanced date parsing for Task 2.4.
        
        Args:
            content: Raw markdown content
            
        Returns:
            Datetime object if found, None otherwise
        """
        # Enhanced date patterns with more formats
        date_patterns = [
            # ISO format dates
            (r'(\d{4}-\d{2}-\d{2})', '%Y-%m-%d'),
            (r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})', '%Y-%m-%d %H:%M'),
            (r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})', '%Y-%m-%d %H:%M:%S'),
            
            # US format dates
            (r'(\d{1,2}/\d{1,2}/\d{4})', '%m/%d/%Y'),
            (r'(\d{1,2}-\d{1,2}-\d{4})', '%m-%d-%Y'),
            
            # European format dates
            (r'(\d{1,2}\.\d{1,2}\.\d{4})', '%d.%m.%Y'),
            
            # Long format dates
            (r'(\d{1,2}\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{4})', '%d %B %Y'),
            (r'((?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},?\s+\d{4})', '%B %d, %Y'),
            
            # Short month format
            (r'(\d{1,2}\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{4})', '%d %b %Y'),
            (r'((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},?\s+\d{4})', '%b %d, %Y'),
        ]
        
        # Look for dates in common contexts
        context_patterns = [
            r'(?:created|written|updated|modified|date|on):\s*([^\n]+)',
            r'(?:created|written|updated|modified|date|on)\s+([^\n]+)',
            r'<!--\s*date:\s*([^\n]+)\s*-->',
            r'^\s*\*\*date\*\*:\s*([^\n]+)',
            r'^\s*date:\s*([^\n]+)',
            r'last updated:\s*([^\n]+)',
            r'version.*?(\d{4}-\d{2}-\d{2})',
        ]
        
        # First try to find dates in context
        for context_pattern in context_patterns:
            context_match = re.search(context_pattern, content, re.IGNORECASE | re.MULTILINE)
            if context_match:
                date_text = context_match.group(1).strip()
                
                # Try to parse the date text with various formats
                for date_pattern, date_format in date_patterns:
                    date_match = re.search(date_pattern, date_text, re.IGNORECASE)
                    if date_match:
                        try:
                            date_str = date_match.group(1)
                            # Handle special cases for format parsing
                            if 'T' in date_str:
                                date_str = date_str.replace('T', ' ')
                            
                            return datetime.strptime(date_str, date_format)
                        except ValueError:
                            continue
        
        # If no contextual dates found, look for any dates in the content
        for date_pattern, date_format in date_patterns:
            matches = re.findall(date_pattern, content, re.IGNORECASE)
            if matches:
                for date_str in matches:
                    try:
                        # Handle special cases for format parsing
                        if 'T' in date_str:
                            date_str = date_str.replace('T', ' ')
                        
                        parsed_date = datetime.strptime(date_str, date_format)
                        
                        # Only return reasonable dates (not too far in the past or future)
                        current_year = datetime.now().year
                        if 2000 <= parsed_date.year <= current_year + 1:
                            return parsed_date
                    except ValueError:
                        continue
        
        return None
    
    def _handle_multi_category_classification(self, filename: str, 
                                            category_matches: List[Tuple[Category, float, str]]) -> Tuple[Category, float]:
        """
        Handle edge cases for multi-category files (Task 2.4).
        
        When a file could belong to multiple categories, this method applies
        business logic to determine the most appropriate classification.
        
        Args:
            filename: Name of the file being classified
            category_matches: List of (category, confidence, pattern) tuples
            
        Returns:
            Tuple of (best_category, adjusted_confidence)
        """
        if len(category_matches) <= 1:
            return category_matches[0][:2] if category_matches else (Category.UNCATEGORIZED, 0.0)
        
        # Sort matches by confidence (highest first)
        sorted_matches = sorted(category_matches, key=lambda x: x[1], reverse=True)
        
        # Define category priority rules for conflicts
        category_priorities = {
            Category.IMPLEMENTATION_COMPLETION: 10,  # Highest priority for completion files
            Category.SETUP_CONFIG: 9,               # Setup files are usually specific
            Category.FEATURE_DOCS: 8,               # Feature docs are important
            Category.TESTING_VALIDATION: 7,         # Testing docs are specific
            Category.INTEGRATION_GUIDES: 6,         # Integration guides
            Category.QUICK_REFERENCES: 5,           # Quick references
            Category.HISTORICAL_ARCHIVE: 4,         # Archive files
            Category.UNCATEGORIZED: 1               # Lowest priority
        }
        
        # Get the top two matches
        first_match = sorted_matches[0]
        second_match = sorted_matches[1] if len(sorted_matches) > 1 else None
        
        first_category, first_confidence, first_pattern = first_match
        
        # If the confidence difference is significant (>0.2), use the highest confidence
        if second_match:
            second_category, second_confidence, second_pattern = second_match
            confidence_diff = first_confidence - second_confidence
            
            if confidence_diff > 0.2:
                self.logger.debug(f"Multi-category file '{filename}': "
                                f"Using {first_category.value} (confidence: {first_confidence:.2f}) "
                                f"over {second_category.value} (confidence: {second_confidence:.2f})")
                return first_category, first_confidence
        
        # Handle specific multi-category scenarios
        categories_involved = {match[0] for match in sorted_matches}
        
        # Scenario 1: Completion file that's also a feature doc
        if (Category.IMPLEMENTATION_COMPLETION in categories_involved and 
            Category.FEATURE_DOCS in categories_involved):
            
            # If it's clearly a completion file, prioritize that
            if any('COMPLETE' in filename.upper() or 'SUMMARY' in filename.upper() 
                   for pattern in [m[2] for m in sorted_matches]):
                return Category.IMPLEMENTATION_COMPLETION, min(first_confidence + 0.1, 1.0)
            else:
                return Category.FEATURE_DOCS, first_confidence
        
        # Scenario 2: Setup file that's also a feature doc
        if (Category.SETUP_CONFIG in categories_involved and 
            Category.FEATURE_DOCS in categories_involved):
            
            # If it has setup-specific patterns, prioritize setup
            if any('SETUP' in filename.upper() or 'INSTALL' in filename.upper() 
                   for pattern in [m[2] for m in sorted_matches]):
                return Category.SETUP_CONFIG, min(first_confidence + 0.1, 1.0)
            else:
                return Category.FEATURE_DOCS, first_confidence
        
        # Scenario 3: Testing file that's also a feature doc
        if (Category.TESTING_VALIDATION in categories_involved and 
            Category.FEATURE_DOCS in categories_involved):
            
            # If it has test-specific patterns, prioritize testing
            if any('TEST' in filename.upper() or 'VALIDATION' in filename.upper() 
                   for pattern in [m[2] for m in sorted_matches]):
                return Category.TESTING_VALIDATION, min(first_confidence + 0.1, 1.0)
            else:
                return Category.FEATURE_DOCS, first_confidence
        
        # Scenario 4: Quick reference that's also something else
        if Category.QUICK_REFERENCES in categories_involved:
            # Quick references usually have lower priority unless very specific
            non_quick_ref_matches = [m for m in sorted_matches if m[0] != Category.QUICK_REFERENCES]
            if non_quick_ref_matches and non_quick_ref_matches[0][1] > 0.6:
                return non_quick_ref_matches[0][:2]
        
        # Scenario 5: Use category priority as tiebreaker
        if second_match and abs(first_confidence - second_confidence) < 0.1:
            # Very close confidence scores, use priority
            first_priority = category_priorities.get(first_category, 0)
            second_priority = category_priorities.get(second_category, 0)
            
            if second_priority > first_priority:
                self.logger.debug(f"Multi-category file '{filename}': "
                                f"Using priority-based decision: {second_category.value} "
                                f"over {first_category.value}")
                return second_category, second_confidence
        
        # Default: return the highest confidence match with slight boost for handling multi-category
        adjusted_confidence = min(first_confidence + 0.05, 1.0)
        
        self.logger.debug(f"Multi-category file '{filename}': "
                        f"Resolved to {first_category.value} (confidence: {adjusted_confidence:.2f}) "
                        f"from {len(category_matches)} possible categories")
        
        return first_category, adjusted_confidence
    
    def _identify_category_groups(self, category: Category, 
                                analyses: List[FileAnalysis]) -> List[ConsolidationGroup]:
        """Identify consolidation groups within a category."""
        groups = []
        
        if category == Category.IMPLEMENTATION_COMPLETION:
            # Group completion files by common prefixes
            groups.extend(self._group_completion_files(analyses))
        
        elif category == Category.FEATURE_DOCS:
            # Group feature files by feature prefix
            groups.extend(self._group_feature_files(analyses))
        
        elif category == Category.TESTING_VALIDATION:
            # Group test files by test type
            groups.extend(self._group_test_files(analyses))
        
        return groups
    
    def _identify_completion_summary_consolidation(self, analyses: List[FileAnalysis]) -> List[ConsolidationGroup]:
        """
        Identify completion summary consolidation opportunities (Task 4.1).
        
        This method implements enhanced logic to identify files that should be
        consolidated into comprehensive implementation histories.
        
        Args:
            analyses: List of all file analyses
            
        Returns:
            List of ConsolidationGroup objects for completion summaries
        """
        completion_files = [
            analysis for analysis in analyses 
            if analysis.category == Category.IMPLEMENTATION_COMPLETION
        ]
        
        if len(completion_files) < 2:
            return []
        
        groups = []
        
        # Enhanced grouping strategies for completion files
        
        # Strategy 1: Group by task sequences (TASK_1, TASK_2, etc.)
        task_sequence_groups = self._group_by_task_sequences(completion_files)
        groups.extend(task_sequence_groups)
        
        # Strategy 2: Group by phase completions
        phase_groups = self._group_by_phases(completion_files)
        groups.extend(phase_groups)
        
        # Strategy 3: Group by feature completion summaries
        feature_completion_groups = self._group_by_feature_completions(completion_files)
        groups.extend(feature_completion_groups)
        
        # Strategy 4: Group miscellaneous completion files
        misc_completion_groups = self._group_miscellaneous_completions(completion_files)
        groups.extend(misc_completion_groups)
        
        return groups
    
    def _identify_feature_consolidation(self, analyses: List[FileAnalysis], 
                                      processed_files: Set[str]) -> List[ConsolidationGroup]:
        """
        Identify feature documentation consolidation opportunities (Task 4.1).
        
        Args:
            analyses: List of all file analyses
            processed_files: Set of already processed file names
            
        Returns:
            List of ConsolidationGroup objects for feature documentation
        """
        feature_files = [
            analysis for analysis in analyses 
            if (analysis.category == Category.FEATURE_DOCS and 
                analysis.filename not in processed_files)
        ]
        
        if len(feature_files) < 2:
            return []
        
        groups = []
        
        # Enhanced feature grouping with semantic analysis
        feature_clusters = self._cluster_features_by_similarity(feature_files)
        
        for cluster_name, cluster_files in feature_clusters.items():
            if len(cluster_files) > 1:
                # Sort by priority and completeness
                cluster_files.sort(key=lambda x: (
                    x.preservation_priority.value,
                    x.metadata.word_count
                ), reverse=True)
                
                primary = cluster_files[0]
                related = [f.filename for f in cluster_files[1:]]
                
                group = ConsolidationGroup(
                    group_id=f"feature_{cluster_name}_consolidation",
                    category=Category.FEATURE_DOCS,
                    primary_file=primary.filename,
                    related_files=related,
                    consolidation_strategy=ConsolidationStrategy.MERGE_TOPICAL,
                    output_filename=f"{cluster_name}_comprehensive_guide.md",
                    cross_references=self._identify_feature_cross_references(cluster_files),
                    preservation_notes=[
                        f"Consolidating {len(cluster_files)} {cluster_name} documentation files",
                        f"Primary file selected based on completeness and priority"
                    ]
                )
                groups.append(group)
        
        return groups
    
    def _identify_setup_consolidation(self, analyses: List[FileAnalysis], 
                                    processed_files: Set[str]) -> List[ConsolidationGroup]:
        """
        Identify setup and configuration consolidation opportunities (Task 4.1).
        
        Args:
            analyses: List of all file analyses
            processed_files: Set of already processed file names
            
        Returns:
            List of ConsolidationGroup objects for setup documentation
        """
        setup_files = [
            analysis for analysis in analyses 
            if (analysis.category == Category.SETUP_CONFIG and 
                analysis.filename not in processed_files)
        ]
        
        if len(setup_files) < 2:
            return []
        
        groups = []
        
        # Group setup files by setup type
        setup_types = {
            'installation': [],
            'configuration': [],
            'deployment': [],
            'environment': [],
            'database': [],
            'general': []
        }
        
        for analysis in setup_files:
            filename_lower = analysis.filename.lower()
            content_topics = [topic.lower() for topic in analysis.metadata.key_topics]
            
            # Classify setup type based on filename and content
            if any(term in filename_lower for term in ['install', 'setup']):
                setup_types['installation'].append(analysis)
            elif any(term in filename_lower for term in ['config', 'configuration']):
                setup_types['configuration'].append(analysis)
            elif any(term in filename_lower for term in ['deploy', 'deployment']):
                setup_types['deployment'].append(analysis)
            elif any(term in filename_lower for term in ['env', 'environment']):
                setup_types['environment'].append(analysis)
            elif any(term in filename_lower for term in ['database', 'db']):
                setup_types['database'].append(analysis)
            else:
                # Check content topics for classification
                if any(topic in content_topics for topic in ['installation', 'install']):
                    setup_types['installation'].append(analysis)
                elif any(topic in content_topics for topic in ['configuration', 'config']):
                    setup_types['configuration'].append(analysis)
                elif any(topic in content_topics for topic in ['deployment', 'deploy']):
                    setup_types['deployment'].append(analysis)
                elif any(topic in content_topics for topic in ['environment', 'env']):
                    setup_types['environment'].append(analysis)
                elif any(topic in content_topics for topic in ['database', 'db']):
                    setup_types['database'].append(analysis)
                else:
                    setup_types['general'].append(analysis)
        
        # Create consolidation groups for setup types with multiple files
        for setup_type, type_files in setup_types.items():
            if len(type_files) > 1:
                # Sort by priority and word count
                type_files.sort(key=lambda x: (
                    x.preservation_priority.value,
                    x.metadata.word_count
                ), reverse=True)
                
                primary = type_files[0]
                related = [f.filename for f in type_files[1:]]
                
                group = ConsolidationGroup(
                    group_id=f"setup_{setup_type}_consolidation",
                    category=Category.SETUP_CONFIG,
                    primary_file=primary.filename,
                    related_files=related,
                    consolidation_strategy=ConsolidationStrategy.MERGE_SEQUENTIAL,
                    output_filename=f"{setup_type}_guide.md",
                    preservation_notes=[
                        f"Consolidating {len(type_files)} {setup_type} setup files",
                        f"Merged in logical sequence for step-by-step guidance"
                    ]
                )
                groups.append(group)
        
        return groups
    
    def _identify_testing_consolidation(self, analyses: List[FileAnalysis], 
                                      processed_files: Set[str]) -> List[ConsolidationGroup]:
        """
        Identify testing documentation consolidation opportunities (Task 4.1).
        
        Args:
            analyses: List of all file analyses
            processed_files: Set of already processed file names
            
        Returns:
            List of ConsolidationGroup objects for testing documentation
        """
        testing_files = [
            analysis for analysis in analyses 
            if (analysis.category == Category.TESTING_VALIDATION and 
                analysis.filename not in processed_files)
        ]
        
        if len(testing_files) < 3:  # Need at least 3 files to justify consolidation
            return []
        
        groups = []
        
        # Group testing files by test type and create index
        test_reports = []
        test_procedures = []
        validation_results = []
        general_testing = []
        
        for analysis in testing_files:
            filename_lower = analysis.filename.lower()
            content_topics = [topic.lower() for topic in analysis.metadata.key_topics]
            
            if any(term in filename_lower for term in ['report', 'result']):
                test_reports.append(analysis)
            elif any(term in filename_lower for term in ['procedure', 'guide', 'how']):
                test_procedures.append(analysis)
            elif any(term in filename_lower for term in ['validation', 'verify']):
                validation_results.append(analysis)
            else:
                general_testing.append(analysis)
        
        # Create testing index consolidation group
        if len(testing_files) >= 3:
            # Sort all testing files by modification date (newest first)
            testing_files.sort(key=lambda x: x.metadata.last_modified, reverse=True)
            
            primary = testing_files[0]
            related = [f.filename for f in testing_files[1:]]
            
            group = ConsolidationGroup(
                group_id="testing_documentation_consolidation",
                category=Category.TESTING_VALIDATION,
                primary_file=primary.filename,
                related_files=related,
                consolidation_strategy=ConsolidationStrategy.CREATE_INDEX,
                output_filename="testing_documentation_index.md",
                preservation_notes=[
                    f"Creating comprehensive testing documentation index",
                    f"Organizing {len(testing_files)} testing-related files",
                    "Grouped by test reports, procedures, and validation results"
                ]
            )
            groups.append(group)
        
        return groups
    
    def _identify_cross_category_consolidation(self, analyses: List[FileAnalysis], 
                                             processed_files: Set[str]) -> List[ConsolidationGroup]:
        """
        Identify cross-category consolidation opportunities (Task 4.1).
        
        Some files from different categories might be related and should be
        consolidated together (e.g., feature setup + feature docs).
        
        Args:
            analyses: List of all file analyses
            processed_files: Set of already processed file names
            
        Returns:
            List of ConsolidationGroup objects for cross-category consolidation
        """
        groups = []
        unprocessed_analyses = [
            analysis for analysis in analyses 
            if analysis.filename not in processed_files
        ]
        
        # Strategy 1: Identify feature-related files across categories
        feature_cross_groups = self._identify_feature_cross_category_groups(unprocessed_analyses)
        groups.extend(feature_cross_groups)
        
        # Strategy 2: Identify quick reference consolidation opportunities
        quick_ref_groups = self._identify_quick_reference_consolidation(unprocessed_analyses)
        groups.extend(quick_ref_groups)
        
        return groups
    
    def _optimize_consolidation_groups(self, groups: List[ConsolidationGroup]) -> List[ConsolidationGroup]:
        """
        Optimize consolidation groups to avoid conflicts and improve efficiency.
        
        Args:
            groups: List of consolidation groups to optimize
            
        Returns:
            Optimized list of consolidation groups
        """
        if not groups:
            return groups
        
        # Remove duplicate groups and resolve conflicts
        optimized_groups = []
        all_files_in_groups = set()
        
        # Sort groups by priority (completion > feature > setup > testing)
        priority_order = {
            Category.IMPLEMENTATION_COMPLETION: 1,
            Category.FEATURE_DOCS: 2,
            Category.SETUP_CONFIG: 3,
            Category.TESTING_VALIDATION: 4,
            Category.INTEGRATION_GUIDES: 5,
            Category.QUICK_REFERENCES: 6
        }
        
        groups.sort(key=lambda g: (
            priority_order.get(g.category, 10),
            -g.total_files  # Prefer larger groups
        ))
        
        for group in groups:
            # Check for file conflicts
            group_files = {group.primary_file} | set(group.related_files)
            
            if not group_files.intersection(all_files_in_groups):
                # No conflicts, add the group
                optimized_groups.append(group)
                all_files_in_groups.update(group_files)
            else:
                # Handle conflicts by splitting or merging groups
                self.logger.debug(f"Resolving conflict for group {group.group_id}")
                # For now, skip conflicting groups (could be enhanced later)
                continue
        
        return optimized_groups
    
    def _group_completion_files(self, analyses: List[FileAnalysis]) -> List[ConsolidationGroup]:
        """
        Group completion files for consolidation.
        Enhanced for Task 2.2 with improved completion file recognition.
        """
        groups = []
        
        # Enhanced grouping by common prefixes and patterns
        prefix_groups = {}
        for analysis in analyses:
            filename = analysis.filename
            filename_upper = filename.upper()
            
            # Extract prefix patterns with enhanced logic
            prefix = None
            
            # Task 2.2: Enhanced completion file pattern recognition
            if re.match(r'^TASK_.*_COMPLETE\.md$', filename, re.IGNORECASE):
                prefix = 'TASK_COMPLETE'
            elif re.match(r'^TASK_\d+.*\.md$', filename, re.IGNORECASE):
                prefix = 'TASK_NUMBERED'
            elif filename_upper.startswith('TASK_'):
                prefix = 'TASK_GENERAL'
            elif re.match(r'^PHASE_.*_COMPLETE\.md$', filename, re.IGNORECASE):
                prefix = 'PHASE_COMPLETE'
            elif filename_upper.startswith('PHASE_'):
                prefix = 'PHASE_GENERAL'
            elif re.match(r'^MILESTONE_.*\.md$', filename, re.IGNORECASE):
                prefix = 'MILESTONE'
            elif re.match(r'^IMPLEMENTATION_.*\.md$', filename, re.IGNORECASE):
                prefix = 'IMPLEMENTATION'
            elif '_COMPLETE' in filename_upper:
                prefix = 'GENERAL_COMPLETE'
            elif 'COMPLETE' in filename_upper:
                prefix = 'COMPLETION'
            elif '_SUMMARY' in filename_upper:
                prefix = 'SUMMARY'
            elif 'SUMMARY' in filename_upper:
                prefix = 'GENERAL_SUMMARY'
            else:
                continue
            
            if prefix not in prefix_groups:
                prefix_groups[prefix] = []
            prefix_groups[prefix].append(analysis)
        
        # Create consolidation groups with enhanced naming
        for prefix, group_analyses in prefix_groups.items():
            if len(group_analyses) > 1:
                # Sort by filename to ensure consistent ordering
                group_analyses.sort(key=lambda x: x.filename)
                
                primary = group_analyses[0]
                related = [a.filename for a in group_analyses[1:]]
                
                # Generate appropriate output filename based on prefix
                output_filename_map = {
                    'TASK_COMPLETE': 'task_completion_summary.md',
                    'TASK_NUMBERED': 'numbered_task_summary.md',
                    'TASK_GENERAL': 'task_implementation_summary.md',
                    'PHASE_COMPLETE': 'phase_completion_summary.md',
                    'PHASE_GENERAL': 'phase_implementation_summary.md',
                    'MILESTONE': 'milestone_summary.md',
                    'IMPLEMENTATION': 'implementation_summary.md',
                    'GENERAL_COMPLETE': 'completion_summary.md',
                    'COMPLETION': 'project_completion_summary.md',
                    'SUMMARY': 'consolidated_summary.md',
                    'GENERAL_SUMMARY': 'general_summary.md'
                }
                
                output_filename = output_filename_map.get(prefix, f"{prefix.lower()}_summary.md")
                
                group = ConsolidationGroup(
                    group_id=f"{prefix.lower()}_consolidation",
                    category=Category.IMPLEMENTATION_COMPLETION,
                    primary_file=primary.filename,
                    related_files=related,
                    consolidation_strategy=ConsolidationStrategy.COMBINE_SUMMARIES,
                    output_filename=output_filename
                )
                groups.append(group)
        
        return groups
    
    def _group_feature_files(self, analyses: List[FileAnalysis]) -> List[ConsolidationGroup]:
        """
        Group feature files for consolidation.
        Enhanced for Task 2.2 with improved feature recognition.
        """
        groups = []
        
        # Enhanced feature grouping with more comprehensive patterns
        feature_groups = {}
        for analysis in analyses:
            filename = analysis.filename
            filename_upper = filename.upper()
            
            # Extract feature prefix with enhanced patterns
            feature = None
            
            # Task 2.2: Enhanced feature prefix detection
            feature_mappings = {
                'PAYMENT': ['PAYMENT_', 'PAY_'],
                'TOURNAMENT': ['TOURNAMENT_', 'TOURNEY_', 'COMPETITION_'],
                'AUTHENTICATION': ['AUTH_', 'AUTHENTICATION_', 'LOGIN_', 'SIGNUP_'],
                'NOTIFICATION': ['NOTIFICATION_', 'NOTIFY_', 'ALERT_'],
                'DASHBOARD': ['DASHBOARD_', 'DASH_', 'OVERVIEW_'],
                'USER': ['USER_', 'PROFILE_', 'ACCOUNT_'],
                'TEAM': ['TEAM_', 'GROUP_'],
                'COACHING': ['COACHING_', 'COACH_', 'MENTOR_'],
                'MESSAGING': ['MESSAGE_', 'MESSAGING_', 'CHAT_'],
                'ADMIN': ['ADMIN_', 'ADMINISTRATION_'],
                'SEARCH': ['SEARCH_', 'FIND_', 'LOOKUP_'],
                'REPORT': ['REPORT_', 'REPORTING_', 'ANALYTICS_']
            }
            
            for feature_name, prefixes in feature_mappings.items():
                for prefix in prefixes:
                    if filename_upper.startswith(prefix):
                        feature = feature_name.lower()
                        break
                if feature:
                    break
            
            # Also check for feature names within filename
            if not feature:
                for feature_name in feature_mappings.keys():
                    if feature_name in filename_upper:
                        feature = feature_name.lower()
                        break
            
            if feature:
                if feature not in feature_groups:
                    feature_groups[feature] = []
                feature_groups[feature].append(analysis)
        
        # Create consolidation groups for features with multiple files
        for feature, group_analyses in feature_groups.items():
            if len(group_analyses) > 1:
                # Sort by filename to ensure consistent primary file selection
                group_analyses.sort(key=lambda x: x.filename)
                
                primary = group_analyses[0]
                related = [a.filename for a in group_analyses[1:]]
                
                group = ConsolidationGroup(
                    group_id=f"{feature}_consolidation",
                    category=Category.FEATURE_DOCS,
                    primary_file=primary.filename,
                    related_files=related,
                    consolidation_strategy=ConsolidationStrategy.MERGE_TOPICAL,
                    output_filename=f"{feature}_guide.md"
                )
                groups.append(group)
        
        return groups
    
    def _group_test_files(self, analyses: List[FileAnalysis]) -> List[ConsolidationGroup]:
        """Group test files for consolidation."""
        # For test files, we typically create an index rather than merge
        if len(analyses) > 3:
            primary = analyses[0]
            related = [a.filename for a in analyses[1:]]
            
            group = ConsolidationGroup(
                group_id="test_consolidation",
                category=Category.TESTING_VALIDATION,
                primary_file=primary.filename,
                related_files=related,
                consolidation_strategy=ConsolidationStrategy.CREATE_INDEX,
                output_filename="testing_index.md"
            )
            return [group]
        
        return []
    
    # Enhanced helper methods for Task 4.1 consolidation group identification
    
    def _group_by_task_sequences(self, completion_files: List[FileAnalysis]) -> List[ConsolidationGroup]:
        """Group completion files by task sequences (TASK_1, TASK_2, etc.)."""
        task_files = {}
        
        for analysis in completion_files:
            filename = analysis.filename
            
            # Extract task numbers and sequences
            task_match = re.search(r'TASK_(\d+)', filename, re.IGNORECASE)
            if task_match:
                task_num = int(task_match.group(1))
                # Group by ranges (1-10, 11-20, etc.)
                task_range = f"tasks_{(task_num-1)//10*10+1}_{min((task_num-1)//10*10+10, 99)}"
                
                if task_range not in task_files:
                    task_files[task_range] = []
                task_files[task_range].append(analysis)
        
        groups = []
        for task_range, files in task_files.items():
            if len(files) > 1:
                files.sort(key=lambda x: x.filename)
                primary = files[0]
                related = [f.filename for f in files[1:]]
                
                group = ConsolidationGroup(
                    group_id=f"{task_range}_consolidation",
                    category=Category.IMPLEMENTATION_COMPLETION,
                    primary_file=primary.filename,
                    related_files=related,
                    consolidation_strategy=ConsolidationStrategy.COMBINE_SUMMARIES,
                    output_filename=f"{task_range}_completion_summary.md"
                )
                groups.append(group)
        
        return groups
    
    def _group_by_phases(self, completion_files: List[FileAnalysis]) -> List[ConsolidationGroup]:
        """Group completion files by project phases."""
        phase_files = {}
        
        for analysis in completion_files:
            filename = analysis.filename
            filename_upper = filename.upper()
            
            # Identify phase-related files
            if 'PHASE' in filename_upper:
                phase_match = re.search(r'PHASE_(\w+)', filename, re.IGNORECASE)
                if phase_match:
                    phase_name = phase_match.group(1).lower()
                    if phase_name not in phase_files:
                        phase_files[phase_name] = []
                    phase_files[phase_name].append(analysis)
            elif any(term in filename_upper for term in ['MILESTONE', 'SPRINT', 'ITERATION']):
                phase_key = 'milestones'
                if phase_key not in phase_files:
                    phase_files[phase_key] = []
                phase_files[phase_key].append(analysis)
        
        groups = []
        for phase_name, files in phase_files.items():
            if len(files) > 1:
                files.sort(key=lambda x: x.metadata.last_modified)
                primary = files[0]
                related = [f.filename for f in files[1:]]
                
                group = ConsolidationGroup(
                    group_id=f"phase_{phase_name}_consolidation",
                    category=Category.IMPLEMENTATION_COMPLETION,
                    primary_file=primary.filename,
                    related_files=related,
                    consolidation_strategy=ConsolidationStrategy.COMBINE_SUMMARIES,
                    output_filename=f"phase_{phase_name}_summary.md"
                )
                groups.append(group)
        
        return groups
    
    def _group_by_feature_completions(self, completion_files: List[FileAnalysis]) -> List[ConsolidationGroup]:
        """Group completion files by feature implementations."""
        feature_completions = {}
        
        for analysis in completion_files:
            filename = analysis.filename
            filename_upper = filename.upper()
            
            # Check for feature-specific completion files
            feature_keywords = ['PAYMENT', 'TOURNAMENT', 'AUTH', 'NOTIFICATION', 'DASHBOARD', 'USER']
            for keyword in feature_keywords:
                if keyword in filename_upper and 'COMPLETE' in filename_upper:
                    feature_key = keyword.lower()
                    if feature_key not in feature_completions:
                        feature_completions[feature_key] = []
                    feature_completions[feature_key].append(analysis)
                    break
        
        groups = []
        for feature_name, files in feature_completions.items():
            if len(files) > 1:
                files.sort(key=lambda x: x.metadata.last_modified)
                primary = files[0]
                related = [f.filename for f in files[1:]]
                
                group = ConsolidationGroup(
                    group_id=f"feature_{feature_name}_completion_consolidation",
                    category=Category.IMPLEMENTATION_COMPLETION,
                    primary_file=primary.filename,
                    related_files=related,
                    consolidation_strategy=ConsolidationStrategy.COMBINE_SUMMARIES,
                    output_filename=f"{feature_name}_implementation_history.md"
                )
                groups.append(group)
        
        return groups
    
    def _group_miscellaneous_completions(self, completion_files: List[FileAnalysis]) -> List[ConsolidationGroup]:
        """Group miscellaneous completion files that don't fit other patterns."""
        # Find completion files that haven't been grouped yet
        misc_files = []
        
        for analysis in completion_files:
            filename = analysis.filename
            filename_upper = filename.upper()
            
            # Check if it's a general completion file
            if ('COMPLETE' in filename_upper or 'SUMMARY' in filename_upper) and not any(
                pattern in filename_upper for pattern in ['TASK_', 'PHASE_', 'PAYMENT', 'TOURNAMENT', 'AUTH']
            ):
                misc_files.append(analysis)
        
        groups = []
        if len(misc_files) > 2:  # Only group if we have several miscellaneous files
            misc_files.sort(key=lambda x: x.metadata.last_modified, reverse=True)
            primary = misc_files[0]
            related = [f.filename for f in misc_files[1:]]
            
            group = ConsolidationGroup(
                group_id="miscellaneous_completion_consolidation",
                category=Category.IMPLEMENTATION_COMPLETION,
                primary_file=primary.filename,
                related_files=related,
                consolidation_strategy=ConsolidationStrategy.COMBINE_SUMMARIES,
                output_filename="general_completion_summary.md"
            )
            groups.append(group)
        
        return groups
    
    def _cluster_features_by_similarity(self, feature_files: List[FileAnalysis]) -> Dict[str, List[FileAnalysis]]:
        """Cluster feature files by semantic similarity and naming patterns."""
        clusters = {}
        
        # Enhanced feature clustering with better pattern recognition
        feature_mappings = {
            'payment': ['payment', 'pay', 'billing', 'stripe', 'transaction'],
            'tournament': ['tournament', 'tourney', 'competition', 'match', 'bracket'],
            'authentication': ['auth', 'login', 'signup', 'user', 'account', 'profile'],
            'notification': ['notification', 'notify', 'alert', 'message', 'email'],
            'dashboard': ['dashboard', 'overview', 'summary', 'stats', 'analytics'],
            'team': ['team', 'group', 'member', 'roster'],
            'coaching': ['coaching', 'coach', 'mentor', 'training'],
            'admin': ['admin', 'administration', 'management', 'control']
        }
        
        for analysis in feature_files:
            filename_lower = analysis.filename.lower()
            content_topics = [topic.lower() for topic in analysis.metadata.key_topics]
            
            # Find the best matching cluster
            best_cluster = None
            best_score = 0
            
            for cluster_name, keywords in feature_mappings.items():
                score = 0
                
                # Score based on filename matches
                for keyword in keywords:
                    if keyword in filename_lower:
                        score += 2  # Filename matches are weighted higher
                
                # Score based on content topic matches
                for topic in content_topics:
                    for keyword in keywords:
                        if keyword in topic:
                            score += 1
                
                if score > best_score:
                    best_score = score
                    best_cluster = cluster_name
            
            # If no good match found, create a cluster based on filename prefix
            if best_cluster is None or best_score < 2:
                # Extract prefix from filename
                prefix_match = re.match(r'^([A-Z]+)_', analysis.filename)
                if prefix_match:
                    prefix = prefix_match.group(1).lower()
                    best_cluster = f"prefix_{prefix}"
                else:
                    best_cluster = "miscellaneous"
            
            if best_cluster not in clusters:
                clusters[best_cluster] = []
            clusters[best_cluster].append(analysis)
        
        # Filter out clusters with only one file
        return {name: files for name, files in clusters.items() if len(files) > 1}
    
    def _identify_feature_cross_references(self, cluster_files: List[FileAnalysis]) -> List[str]:
        """Identify cross-references between feature files."""
        cross_refs = []
        
        for analysis in cluster_files:
            # Add internal links as cross-references
            cross_refs.extend(analysis.metadata.internal_links)
            
            # Add topic-based cross-references
            for topic in analysis.metadata.key_topics:
                if topic.lower() in ['api', 'integration', 'setup', 'configuration']:
                    cross_refs.append(f"Related: {topic} documentation")
        
        # Remove duplicates and return
        return list(set(cross_refs))
    
    def _identify_feature_cross_category_groups(self, analyses: List[FileAnalysis]) -> List[ConsolidationGroup]:
        """Identify feature-related files across different categories."""
        groups = []
        
        # Group files by feature keywords across categories
        feature_groups = {}
        
        for analysis in analyses:
            filename_lower = analysis.filename.lower()
            
            # Identify feature keywords
            feature_keywords = ['payment', 'tournament', 'auth', 'notification', 'dashboard']
            for keyword in feature_keywords:
                if keyword in filename_lower:
                    if keyword not in feature_groups:
                        feature_groups[keyword] = []
                    feature_groups[keyword].append(analysis)
                    break
        
        # Create cross-category consolidation groups
        for feature_name, feature_analyses in feature_groups.items():
            if len(feature_analyses) > 2:  # Need multiple files from different categories
                categories = set(analysis.category for analysis in feature_analyses)
                if len(categories) > 1:  # Files from different categories
                    # Sort by category priority and word count
                    feature_analyses.sort(key=lambda x: (
                        x.category.value,
                        -x.metadata.word_count
                    ))
                    
                    primary = feature_analyses[0]
                    related = [f.filename for f in feature_analyses[1:]]
                    
                    group = ConsolidationGroup(
                        group_id=f"cross_category_{feature_name}_consolidation",
                        category=primary.category,
                        primary_file=primary.filename,
                        related_files=related,
                        consolidation_strategy=ConsolidationStrategy.MERGE_TOPICAL,
                        output_filename=f"{feature_name}_complete_guide.md",
                        preservation_notes=[
                            f"Cross-category consolidation for {feature_name}",
                            f"Includes files from categories: {', '.join(c.value for c in categories)}"
                        ]
                    )
                    groups.append(group)
        
        return groups
    
    def _identify_quick_reference_consolidation(self, analyses: List[FileAnalysis]) -> List[ConsolidationGroup]:
        """Identify quick reference consolidation opportunities."""
        quick_ref_files = [
            analysis for analysis in analyses 
            if (analysis.category == Category.QUICK_REFERENCES or
                'quick' in analysis.filename.lower() or
                'reference' in analysis.filename.lower() or
                'guide' in analysis.filename.lower())
        ]
        
        if len(quick_ref_files) > 2:
            # Sort by word count (shorter files first for quick references)
            quick_ref_files.sort(key=lambda x: x.metadata.word_count)
            
            primary = quick_ref_files[0]
            related = [f.filename for f in quick_ref_files[1:]]
            
            group = ConsolidationGroup(
                group_id="quick_reference_consolidation",
                category=Category.QUICK_REFERENCES,
                primary_file=primary.filename,
                related_files=related,
                consolidation_strategy=ConsolidationStrategy.CREATE_INDEX,
                output_filename="quick_reference_index.md",
                preservation_notes=[
                    f"Consolidating {len(quick_ref_files)} quick reference files",
                    "Creating comprehensive quick reference index"
                ]
            )
            return [group]
        
        return []
    
    # Task 6.1: Outdated Content Detection Methods
    # Requirements: 7.1, 7.2, 7.5
    
    def identify_outdated_content(self, analyses: List[FileAnalysis]) -> Dict[str, List[FileAnalysis]]:
        """
        Identify potentially outdated content based on timestamps and references.
        
        Implements Requirement 7.1: Identify potentially outdated content based on 
        timestamps and references.
        
        Args:
            analyses: List of FileAnalysis objects for all discovered files
            
        Returns:
            Dictionary categorizing files by outdated status:
            - 'potentially_outdated': Files that appear outdated based on timestamps
            - 'superseded': Files that may be superseded by newer versions
            - 'removal_candidates': Files that can be safely removed
            - 'archive_candidates': Files that should be archived but preserved
        """
        self.logger.info("Identifying outdated content based on timestamps and references")
        
        outdated_content = {
            'potentially_outdated': [],
            'superseded': [],
            'removal_candidates': [],
            'archive_candidates': []
        }
        
        # Sort analyses by modification date for comparison
        sorted_analyses = sorted(analyses, key=lambda x: x.metadata.last_modified or datetime.min)
        
        # Identify potentially outdated files based on timestamps
        outdated_content['potentially_outdated'] = self._identify_timestamp_outdated(analyses)
        
        # Identify superseded files (newer versions exist)
        outdated_content['superseded'] = self._identify_superseded_files(analyses)
        
        # Identify removal candidates
        outdated_content['removal_candidates'] = self._identify_removal_candidates(analyses)
        
        # Identify archive candidates
        outdated_content['archive_candidates'] = self._identify_archive_candidates(analyses)
        
        # Log summary
        total_outdated = sum(len(files) for files in outdated_content.values())
        self.logger.info(f"Identified {total_outdated} files with outdated content issues:")
        for category, files in outdated_content.items():
            if files:
                self.logger.info(f"  {category}: {len(files)} files")
        
        return outdated_content
    
    def _identify_timestamp_outdated(self, analyses: List[FileAnalysis]) -> List[FileAnalysis]:
        """
        Identify files that are potentially outdated based on timestamps.
        
        Implements Requirement 7.1: Analyze file timestamps to identify potentially outdated content.
        
        Args:
            analyses: List of FileAnalysis objects
            
        Returns:
            List of FileAnalysis objects for potentially outdated files
        """
        outdated_files = []
        current_time = datetime.now()
        
        # Define age thresholds for different content types
        age_thresholds = {
            ContentType.COMPLETION_SUMMARY: 365,  # 1 year for completion summaries
            ContentType.FEATURE_GUIDE: 180,      # 6 months for feature guides
            ContentType.SETUP_PROCEDURE: 90,     # 3 months for setup procedures
            ContentType.TEST_REPORT: 30,         # 1 month for test reports
            ContentType.QUICK_REFERENCE: 180,    # 6 months for quick references
            ContentType.INTEGRATION_GUIDE: 120,  # 4 months for integration guides
            ContentType.HISTORICAL_DOC: 0,       # Historical docs are expected to be old
            ContentType.GENERAL_DOC: 365         # 1 year for general docs
        }
        
        for analysis in analyses:
            if not analysis.metadata.last_modified:
                continue
            
            # Calculate file age in days
            age_days = (current_time - analysis.metadata.last_modified).days
            
            # Get threshold for this content type
            threshold = age_thresholds.get(analysis.content_type, 365)
            
            # Skip historical documents as they're expected to be old
            if analysis.content_type == ContentType.HISTORICAL_DOC:
                continue
            
            # Check if file exceeds age threshold
            if age_days > threshold:
                analysis.processing_notes.append(
                    f"Potentially outdated: {age_days} days old (threshold: {threshold} days)"
                )
                outdated_files.append(analysis)
                self.logger.debug(f"Identified outdated file: {analysis.filename} "
                                f"({age_days} days old, threshold: {threshold})")
        
        return outdated_files
    
    def _identify_superseded_files(self, analyses: List[FileAnalysis]) -> List[FileAnalysis]:
        """
        Identify files that may be superseded by newer versions.
        
        Implements Requirement 7.2: Detect version conflicts and superseded files.
        
        Args:
            analyses: List of FileAnalysis objects
            
        Returns:
            List of FileAnalysis objects for superseded files
        """
        superseded_files = []
        
        # Group files by base name to find version conflicts
        file_groups = {}
        for analysis in analyses:
            base_name = self._extract_base_filename(analysis.filename)
            if base_name not in file_groups:
                file_groups[base_name] = []
            file_groups[base_name].append(analysis)
        
        # Check each group for version conflicts
        for base_name, group in file_groups.items():
            if len(group) <= 1:
                continue
            
            # Sort by modification date (newest first)
            sorted_group = sorted(group, 
                                key=lambda x: x.metadata.last_modified or datetime.min, 
                                reverse=True)
            
            # Check for version patterns
            versioned_files = []
            for analysis in sorted_group:
                version_info = self._extract_version_from_filename(analysis.filename)
                if version_info:
                    versioned_files.append((analysis, version_info))
            
            # If we have versioned files, mark older versions as superseded
            if len(versioned_files) > 1:
                # Sort by version (newest first)
                versioned_files.sort(key=lambda x: x[1], reverse=True)
                
                # Mark all but the newest as superseded
                for analysis, version in versioned_files[1:]:
                    analysis.processing_notes.append(
                        f"Superseded by newer version: {versioned_files[0][0].filename}"
                    )
                    superseded_files.append(analysis)
                    self.logger.debug(f"Identified superseded file: {analysis.filename} "
                                    f"(version {version}, superseded by {versioned_files[0][1]})")
            
            # Check for completion files with similar names
            completion_files = [a for a in sorted_group 
                              if 'complete' in a.filename.lower() or 'summary' in a.filename.lower()]
            
            if len(completion_files) > 1:
                # Mark older completion files as superseded
                for analysis in completion_files[1:]:
                    analysis.processing_notes.append(
                        f"Superseded by newer completion file: {completion_files[0].filename}"
                    )
                    superseded_files.append(analysis)
                    self.logger.debug(f"Identified superseded completion file: {analysis.filename}")
        
        return superseded_files
    
    def _identify_removal_candidates(self, analyses: List[FileAnalysis]) -> List[FileAnalysis]:
        """
        Identify files that can be safely removed after consolidation.
        
        Implements Requirement 7.5: Suggest which files can be safely removed.
        
        Args:
            analyses: List of FileAnalysis objects
            
        Returns:
            List of FileAnalysis objects for removal candidates
        """
        removal_candidates = []
        
        for analysis in analyses:
            should_remove = False
            removal_reason = ""
            
            # Check for empty or minimal content files
            if analysis.metadata.word_count < 10:
                should_remove = True
                removal_reason = f"Minimal content ({analysis.metadata.word_count} words)"
            
            # Check for duplicate content (very similar files)
            elif self._is_duplicate_content(analysis, analyses):
                should_remove = True
                removal_reason = "Duplicate content detected"
            
            # Check for temporary or draft files
            elif self._is_temporary_file(analysis.filename):
                should_remove = True
                removal_reason = "Temporary or draft file"
            
            # Check for very old test reports or logs
            elif (analysis.content_type == ContentType.TEST_REPORT and 
                  analysis.metadata.last_modified and
                  (datetime.now() - analysis.metadata.last_modified).days > 90):
                should_remove = True
                removal_reason = "Old test report (>90 days)"
            
            if should_remove:
                analysis.processing_notes.append(f"Removal candidate: {removal_reason}")
                removal_candidates.append(analysis)
                self.logger.debug(f"Identified removal candidate: {analysis.filename} ({removal_reason})")
        
        return removal_candidates
    
    def _identify_archive_candidates(self, analyses: List[FileAnalysis]) -> List[FileAnalysis]:
        """
        Identify files that should be archived but preserved.
        
        Implements Requirement 7.5: Flag files for archival.
        
        Args:
            analyses: List of FileAnalysis objects
            
        Returns:
            List of FileAnalysis objects for archive candidates
        """
        archive_candidates = []
        
        for analysis in analyses:
            should_archive = False
            archive_reason = ""
            
            # Archive very old completion summaries
            if (analysis.content_type == ContentType.COMPLETION_SUMMARY and
                analysis.metadata.last_modified and
                (datetime.now() - analysis.metadata.last_modified).days > 365):
                should_archive = True
                archive_reason = "Old completion summary (>1 year)"
            
            # Archive deprecated feature documentation
            elif self._is_deprecated_content(analysis):
                should_archive = True
                archive_reason = "Deprecated feature documentation"
            
            # Archive old setup procedures that may still have reference value
            elif (analysis.content_type == ContentType.SETUP_PROCEDURE and
                  analysis.metadata.last_modified and
                  (datetime.now() - analysis.metadata.last_modified).days > 180):
                should_archive = True
                archive_reason = "Old setup procedure (>6 months)"
            
            # Archive files with historical value but outdated information
            elif (analysis.preservation_priority == Priority.LOW and
                  analysis.metadata.last_modified and
                  (datetime.now() - analysis.metadata.last_modified).days > 180):
                should_archive = True
                archive_reason = "Low priority historical content"
            
            if should_archive:
                analysis.processing_notes.append(f"Archive candidate: {archive_reason}")
                archive_candidates.append(analysis)
                self.logger.debug(f"Identified archive candidate: {analysis.filename} ({archive_reason})")
        
        return archive_candidates
    
    def _extract_base_filename(self, filename: str) -> str:
        """Extract base filename without version indicators."""
        import re
        
        # Remove common version patterns
        base_name = re.sub(r'_v\d+', '', filename, flags=re.IGNORECASE)
        base_name = re.sub(r'_version_\d+', '', base_name, flags=re.IGNORECASE)
        base_name = re.sub(r'_\d{4}-\d{2}-\d{2}', '', base_name)
        base_name = re.sub(r'_\d+$', '', base_name.replace('.md', ''))
        
        return base_name
    
    def _extract_version_from_filename(self, filename: str) -> Optional[str]:
        """Extract version information from filename."""
        import re
        
        version_patterns = [
            r'_v(\d+(?:\.\d+)*)',
            r'_version_(\d+(?:\.\d+)*)',
            r'_(\d{4}-\d{2}-\d{2})',
            r'_(\d+)\.md$'
        ]
        
        for pattern in version_patterns:
            match = re.search(pattern, filename, re.IGNORECASE)
            if match:
                return match.group(1)
        
        return None
    
    def _is_duplicate_content(self, analysis: FileAnalysis, all_analyses: List[FileAnalysis]) -> bool:
        """Check if file content is duplicate of another file."""
        if analysis.metadata.word_count < 20:  # Too small to reliably detect duplicates
            return False
        
        current_topics = set(analysis.metadata.key_topics)
        
        for other_analysis in all_analyses:
            if other_analysis.filename == analysis.filename:
                continue
            
            # Compare word counts (similar size)
            word_count_ratio = min(analysis.metadata.word_count, other_analysis.metadata.word_count) / \
                             max(analysis.metadata.word_count, other_analysis.metadata.word_count)
            
            if word_count_ratio < 0.8:  # Different sizes, likely not duplicates
                continue
            
            # Compare topics
            other_topics = set(other_analysis.metadata.key_topics)
            if current_topics and other_topics:
                topic_overlap = len(current_topics & other_topics) / len(current_topics | other_topics)
                if topic_overlap > 0.8:  # 80% topic overlap
                    return True
        
        return False
    
    def _is_temporary_file(self, filename: str) -> bool:
        """Check if filename indicates a temporary or draft file."""
        temp_indicators = [
            'temp', 'tmp', 'draft', 'backup', 'copy', 'old', 'bak',
            'test_', '_test', 'scratch', 'notes', 'wip'
        ]
        
        filename_lower = filename.lower()
        return any(indicator in filename_lower for indicator in temp_indicators)
    
    def _is_deprecated_content(self, analysis: FileAnalysis) -> bool:
        """Check if content appears to be deprecated."""
        deprecated_indicators = [
            'deprecated', 'obsolete', 'legacy', 'old_', 'archived',
            'removed', 'discontinued', 'superseded'
        ]
        
        filename_lower = analysis.filename.lower()
        
        # Check filename for deprecated indicators
        if any(indicator in filename_lower for indicator in deprecated_indicators):
            return True
        
        # Check topics for deprecated indicators
        topics_text = ' '.join(analysis.metadata.key_topics).lower()
        if any(indicator in topics_text for indicator in deprecated_indicators):
            return True
        
        return False
    
    def detect_version_conflicts(self, analyses: List[FileAnalysis]) -> Dict[str, List[FileAnalysis]]:
        """
        Detect version conflicts between files.
        
        Implements Requirement 7.2: Detect version conflicts and superseded files.
        
        Args:
            analyses: List of FileAnalysis objects
            
        Returns:
            Dictionary mapping conflict groups to lists of conflicting files
        """
        self.logger.info("Detecting version conflicts between files")
        
        conflicts = {}
        
        # Group files by similar base names
        base_name_groups = {}
        for analysis in analyses:
            base_name = self._extract_base_filename(analysis.filename)
            if base_name not in base_name_groups:
                base_name_groups[base_name] = []
            base_name_groups[base_name].append(analysis)
        
        # Check each group for conflicts
        for base_name, group in base_name_groups.items():
            if len(group) <= 1:
                continue
            
            # Look for version conflicts
            versioned_files = []
            completion_files = []
            similar_files = []
            
            for analysis in group:
                if self._extract_version_from_filename(analysis.filename):
                    versioned_files.append(analysis)
                elif 'complete' in analysis.filename.lower() or 'summary' in analysis.filename.lower():
                    completion_files.append(analysis)
                else:
                    similar_files.append(analysis)
            
            # Report version conflicts
            if len(versioned_files) > 1:
                conflict_key = f"version_conflict_{base_name}"
                conflicts[conflict_key] = versioned_files
                self.logger.warning(f"Version conflict detected for {base_name}: "
                                  f"{[f.filename for f in versioned_files]}")
            
            # Report completion file conflicts
            if len(completion_files) > 1:
                conflict_key = f"completion_conflict_{base_name}"
                conflicts[conflict_key] = completion_files
                self.logger.warning(f"Completion file conflict detected for {base_name}: "
                                  f"{[f.filename for f in completion_files]}")
            
            # Report similar file conflicts
            if len(similar_files) > 1:
                # Check if they're actually similar content
                if self._are_files_similar_content(similar_files):
                    conflict_key = f"content_conflict_{base_name}"
                    conflicts[conflict_key] = similar_files
                    self.logger.warning(f"Content conflict detected for {base_name}: "
                                      f"{[f.filename for f in similar_files]}")
        
        self.logger.info(f"Detected {len(conflicts)} version conflicts")
        return conflicts
    
    def create_freshness_indicators(self, analyses: List[FileAnalysis]) -> Dict[str, str]:
        """
        Create freshness indicators for content.
        
        Implements Requirement 7.4: Provide clear indicators about content freshness.
        
        Args:
            analyses: List of FileAnalysis objects
            
        Returns:
            Dictionary mapping filenames to freshness indicators
        """
        self.logger.info("Creating freshness indicators for content")
        
        freshness_indicators = {}
        current_time = datetime.now()
        
        for analysis in analyses:
            if not analysis.metadata.last_modified:
                freshness_indicators[analysis.filename] = " Unknown age"
                continue
            
            age_days = (current_time - analysis.metadata.last_modified).days
            
            # Create freshness indicator based on age and content type
            if age_days <= 7:
                indicator = " Fresh (updated within 1 week)"
            elif age_days <= 30:
                indicator = " Recent (updated within 1 month)"
            elif age_days <= 90:
                indicator = " Aging (updated within 3 months)"
            elif age_days <= 180:
                indicator = " Stale (updated within 6 months)"
            else:
                indicator = f" Old (last updated {age_days} days ago)"
            
            # Add content-specific context
            if analysis.content_type == ContentType.COMPLETION_SUMMARY:
                if age_days > 365:
                    indicator += " - Consider archiving"
                elif age_days > 180:
                    indicator += " - May need review"
            
            elif analysis.content_type == ContentType.SETUP_PROCEDURE:
                if age_days > 90:
                    indicator += " - Verify current accuracy"
                elif age_days > 30:
                    indicator += " - Check for updates"
            
            elif analysis.content_type == ContentType.TEST_REPORT:
                if age_days > 30:
                    indicator += " - Likely outdated"
                elif age_days > 7:
                    indicator += " - May be outdated"
            
            elif analysis.content_type == ContentType.FEATURE_GUIDE:
                if age_days > 180:
                    indicator += " - Verify feature status"
                elif age_days > 90:
                    indicator += " - Check for changes"
            
            freshness_indicators[analysis.filename] = indicator
        
        self.logger.info(f"Created freshness indicators for {len(freshness_indicators)} files")
        return freshness_indicators
    
    def flag_for_archival_or_removal(self, analyses: List[FileAnalysis]) -> Dict[str, Dict[str, List[FileAnalysis]]]:
        """
        Flag files for archival or removal with detailed recommendations.
        
        Implements Requirement 7.5: Flag files for archival or removal.
        
        Args:
            analyses: List of FileAnalysis objects
            
        Returns:
            Dictionary with archival and removal recommendations
        """
        self.logger.info("Flagging files for archival or removal")
        
        recommendations = {
            'archive': {
                'high_priority': [],
                'medium_priority': [],
                'low_priority': []
            },
            'remove': {
                'safe_to_remove': [],
                'review_required': [],
                'keep_but_consolidate': []
            }
        }
        
        for analysis in analyses:
            # Determine archival recommendations
            archive_priority = self._determine_archive_priority(analysis)
            if archive_priority:
                recommendations['archive'][archive_priority].append(analysis)
            
            # Determine removal recommendations
            removal_category = self._determine_removal_category(analysis)
            if removal_category:
                recommendations['remove'][removal_category].append(analysis)
        
        # Log summary
        total_archive = sum(len(files) for files in recommendations['archive'].values())
        total_remove = sum(len(files) for files in recommendations['remove'].values())
        
        self.logger.info(f"Archival recommendations: {total_archive} files")
        for priority, files in recommendations['archive'].items():
            if files:
                self.logger.info(f"  {priority}: {len(files)} files")
        
        self.logger.info(f"Removal recommendations: {total_remove} files")
        for category, files in recommendations['remove'].items():
            if files:
                self.logger.info(f"  {category}: {len(files)} files")
        
        return recommendations
    
    def _are_files_similar_content(self, files: List[FileAnalysis]) -> bool:
        """Check if files have similar content."""
        if len(files) < 2:
            return False
        
        # Compare the first file with all others
        base_file = files[0]
        base_topics = set(base_file.metadata.key_topics)
        
        for other_file in files[1:]:
            other_topics = set(other_file.metadata.key_topics)
            
            if base_topics and other_topics:
                overlap = len(base_topics & other_topics) / len(base_topics | other_topics)
                if overlap > 0.6:  # 60% topic overlap
                    return True
        
        return False
    
    def _determine_archive_priority(self, analysis: FileAnalysis) -> Optional[str]:
        """Determine archive priority for a file."""
        if not analysis.metadata.last_modified:
            return None
        
        age_days = (datetime.now() - analysis.metadata.last_modified).days
        
        # High priority archival
        if (analysis.content_type == ContentType.COMPLETION_SUMMARY and age_days > 365):
            return 'high_priority'
        
        if (analysis.content_type == ContentType.TEST_REPORT and age_days > 90):
            return 'high_priority'
        
        # Medium priority archival
        if (analysis.content_type == ContentType.SETUP_PROCEDURE and age_days > 180):
            return 'medium_priority'
        
        if (analysis.content_type == ContentType.FEATURE_GUIDE and age_days > 365):
            return 'medium_priority'
        
        # Low priority archival
        if (analysis.preservation_priority == Priority.LOW and age_days > 180):
            return 'low_priority'
        
        if self._is_deprecated_content(analysis):
            return 'medium_priority'
        
        return None
    
    def _determine_removal_category(self, analysis: FileAnalysis) -> Optional[str]:
        """Determine removal category for a file."""
        # Safe to remove
        if analysis.metadata.word_count < 10:
            return 'safe_to_remove'
        
        if self._is_temporary_file(analysis.filename):
            return 'safe_to_remove'
        
        if (analysis.content_type == ContentType.TEST_REPORT and 
            analysis.metadata.last_modified and
            (datetime.now() - analysis.metadata.last_modified).days > 90):
            return 'safe_to_remove'
        
        # Review required
        if self._is_duplicate_content(analysis, []):  # Would need full list in real implementation
            return 'review_required'
        
        # Keep but consolidate
        if (analysis.content_type == ContentType.COMPLETION_SUMMARY and
            len(analysis.metadata.key_topics) > 0):
            return 'keep_but_consolidate'
        
        return None
    
    # Task 6.1: Outdated Content Detection Methods
    # Requirements: 7.1, 7.2, 7.5
    
    def identify_outdated_content(self, analyses: List[FileAnalysis]) -> Dict[str, List[FileAnalysis]]:
        """
        Identify potentially outdated content based on timestamps and references.
        
        Implements Requirement 7.1: Identify potentially outdated content based on 
        timestamps and references.
        
        Args:
            analyses: List of FileAnalysis objects for all discovered files
            
        Returns:
            Dictionary categorizing files by outdated status:
            - 'potentially_outdated': Files that appear outdated based on timestamps
            - 'superseded': Files that may be superseded by newer versions
            - 'removal_candidates': Files that can be safely removed
            - 'archive_candidates': Files that should be archived but preserved
        """
        self.logger.info("Identifying outdated content based on timestamps and references")
        
        outdated_content = {
            'potentially_outdated': [],
            'superseded': [],
            'removal_candidates': [],
            'archive_candidates': []
        }
        
        # Sort analyses by modification date for comparison
        sorted_analyses = sorted(analyses, key=lambda x: x.metadata.last_modified or datetime.min)
        
        # Identify potentially outdated files based on timestamps
        outdated_content['potentially_outdated'] = self._identify_timestamp_outdated(analyses)
        
        # Identify superseded files (newer versions exist)
        outdated_content['superseded'] = self._identify_superseded_files(analyses)
        
        # Identify removal candidates
        outdated_content['removal_candidates'] = self._identify_removal_candidates(analyses)
        
        # Identify archive candidates
        outdated_content['archive_candidates'] = self._identify_archive_candidates(analyses)
        
        # Log summary
        total_outdated = sum(len(files) for files in outdated_content.values())
        self.logger.info(f"Identified {total_outdated} files with outdated content issues:")
        for category, files in outdated_content.items():
            if files:
                self.logger.info(f"  {category}: {len(files)} files")
        
        return outdated_content
    
    def _identify_timestamp_outdated(self, analyses: List[FileAnalysis]) -> List[FileAnalysis]:
        """
        Identify files that are potentially outdated based on timestamps.
        
        Implements Requirement 7.1: Analyze file timestamps to identify potentially outdated content.
        
        Args:
            analyses: List of FileAnalysis objects
            
        Returns:
            List of FileAnalysis objects for potentially outdated files
        """
        outdated_files = []
        current_time = datetime.now()
        
        # Define age thresholds for different content types
        age_thresholds = {
            ContentType.COMPLETION_SUMMARY: 365,  # 1 year for completion summaries
            ContentType.FEATURE_GUIDE: 180,      # 6 months for feature guides
            ContentType.SETUP_PROCEDURE: 90,     # 3 months for setup procedures
            ContentType.TEST_REPORT: 30,         # 1 month for test reports
            ContentType.QUICK_REFERENCE: 180,    # 6 months for quick references
            ContentType.INTEGRATION_GUIDE: 120,  # 4 months for integration guides
            ContentType.HISTORICAL_DOC: 0,       # Historical docs are expected to be old
            ContentType.GENERAL_DOC: 365         # 1 year for general docs
        }
        
        for analysis in analyses:
            if not analysis.metadata.last_modified:
                continue
            
            # Calculate file age in days
            age_days = (current_time - analysis.metadata.last_modified).days
            
            # Get threshold for this content type
            threshold = age_thresholds.get(analysis.content_type, 365)
            
            # Skip historical documents as they're expected to be old
            if analysis.content_type == ContentType.HISTORICAL_DOC:
                continue
            
            # Check if file exceeds age threshold
            if age_days > threshold:
                analysis.processing_notes.append(
                    f"Potentially outdated: {age_days} days old (threshold: {threshold} days)"
                )
                outdated_files.append(analysis)
                self.logger.debug(f"Identified outdated file: {analysis.filename} "
                                f"({age_days} days old, threshold: {threshold})")
        
        return outdated_files
    
    def _identify_superseded_files(self, analyses: List[FileAnalysis]) -> List[FileAnalysis]:
        """
        Identify files that may be superseded by newer versions.
        
        Implements Requirement 7.2: Detect version conflicts and superseded files.
        
        Args:
            analyses: List of FileAnalysis objects
            
        Returns:
            List of FileAnalysis objects for superseded files
        """
        superseded_files = []
        
        # Group files by base name to find version conflicts
        file_groups = {}
        for analysis in analyses:
            base_name = self._extract_base_filename(analysis.filename)
            if base_name not in file_groups:
                file_groups[base_name] = []
            file_groups[base_name].append(analysis)
        
        # Check each group for version conflicts
        for base_name, group in file_groups.items():
            if len(group) <= 1:
                continue
            
            # Sort by modification date (newest first)
            sorted_group = sorted(group, 
                                key=lambda x: x.metadata.last_modified or datetime.min, 
                                reverse=True)
            
            # Check for version patterns
            versioned_files = []
            for analysis in sorted_group:
                version_info = self._extract_version_from_filename(analysis.filename)
                if version_info:
                    versioned_files.append((analysis, version_info))
            
            # If we have versioned files, mark older versions as superseded
            if len(versioned_files) > 1:
                # Sort by version (newest first)
                versioned_files.sort(key=lambda x: x[1], reverse=True)
                
                # Mark all but the newest as superseded
                for analysis, version in versioned_files[1:]:
                    analysis.processing_notes.append(
                        f"Superseded by newer version: {versioned_files[0][0].filename}"
                    )
                    superseded_files.append(analysis)
                    self.logger.debug(f"Identified superseded file: {analysis.filename} "
                                    f"(version {version}, superseded by {versioned_files[0][1]})")
            
            # Check for completion files with similar names
            completion_files = [a for a in sorted_group 
                              if 'complete' in a.filename.lower() or 'summary' in a.filename.lower()]
            
            if len(completion_files) > 1:
                # Mark older completion files as superseded
                for analysis in completion_files[1:]:
                    analysis.processing_notes.append(
                        f"Superseded by newer completion file: {completion_files[0].filename}"
                    )
                    superseded_files.append(analysis)
                    self.logger.debug(f"Identified superseded completion file: {analysis.filename}")
        
        return superseded_files
    
    def _identify_removal_candidates(self, analyses: List[FileAnalysis]) -> List[FileAnalysis]:
        """
        Identify files that can be safely removed after consolidation.
        
        Implements Requirement 7.5: Suggest which files can be safely removed.
        
        Args:
            analyses: List of FileAnalysis objects
            
        Returns:
            List of FileAnalysis objects for removal candidates
        """
        removal_candidates = []
        
        for analysis in analyses:
            should_remove = False
            removal_reason = ""
            
            # Check for empty or minimal content files
            if analysis.metadata.word_count < 10:
                should_remove = True
                removal_reason = f"Minimal content ({analysis.metadata.word_count} words)"
            
            # Check for duplicate content (very similar files)
            elif self._is_duplicate_content(analysis, analyses):
                should_remove = True
                removal_reason = "Duplicate content detected"
            
            # Check for temporary or draft files
            elif self._is_temporary_file(analysis.filename):
                should_remove = True
                removal_reason = "Temporary or draft file"
            
            # Check for very old test reports or logs
            elif (analysis.content_type == ContentType.TEST_REPORT and 
                  analysis.metadata.last_modified and
                  (datetime.now() - analysis.metadata.last_modified).days > 90):
                should_remove = True
                removal_reason = "Old test report (>90 days)"
            
            if should_remove:
                analysis.processing_notes.append(f"Removal candidate: {removal_reason}")
                removal_candidates.append(analysis)
                self.logger.debug(f"Identified removal candidate: {analysis.filename} ({removal_reason})")
        
        return removal_candidates
    
    def _identify_archive_candidates(self, analyses: List[FileAnalysis]) -> List[FileAnalysis]:
        """
        Identify files that should be archived but preserved.
        
        Implements Requirement 7.5: Flag files for archival.
        
        Args:
            analyses: List of FileAnalysis objects
            
        Returns:
            List of FileAnalysis objects for archive candidates
        """
        archive_candidates = []
        
        for analysis in analyses:
            should_archive = False
            archive_reason = ""
            
            # Archive very old completion summaries
            if (analysis.content_type == ContentType.COMPLETION_SUMMARY and
                analysis.metadata.last_modified and
                (datetime.now() - analysis.metadata.last_modified).days > 365):
                should_archive = True
                archive_reason = "Old completion summary (>1 year)"
            
            # Archive deprecated feature documentation
            elif self._is_deprecated_content(analysis):
                should_archive = True
                archive_reason = "Deprecated feature documentation"
            
            # Archive old setup procedures that may still have reference value
            elif (analysis.content_type == ContentType.SETUP_PROCEDURE and
                  analysis.metadata.last_modified and
                  (datetime.now() - analysis.metadata.last_modified).days > 180):
                should_archive = True
                archive_reason = "Old setup procedure (>6 months)"
            
            # Archive files with historical value but outdated information
            elif (analysis.preservation_priority == Priority.LOW and
                  analysis.metadata.last_modified and
                  (datetime.now() - analysis.metadata.last_modified).days > 180):
                should_archive = True
                archive_reason = "Low priority historical content"
            
            if should_archive:
                analysis.processing_notes.append(f"Archive candidate: {archive_reason}")
                archive_candidates.append(analysis)
                self.logger.debug(f"Identified archive candidate: {analysis.filename} ({archive_reason})")
        
        return archive_candidates
    
    def _extract_base_filename(self, filename: str) -> str:
        """Extract base filename without version indicators."""
        import re
        
        # Remove common version patterns
        base_name = re.sub(r'_v\d+', '', filename, flags=re.IGNORECASE)
        base_name = re.sub(r'_version_\d+', '', base_name, flags=re.IGNORECASE)
        base_name = re.sub(r'_\d{4}-\d{2}-\d{2}', '', base_name)
        base_name = re.sub(r'_\d+$', '', base_name.replace('.md', ''))
        
        return base_name
    
    def _extract_version_from_filename(self, filename: str) -> Optional[str]:
        """Extract version information from filename."""
        import re
        
        version_patterns = [
            r'_v(\d+(?:\.\d+)*)',
            r'_version_(\d+(?:\.\d+)*)',
            r'_(\d{4}-\d{2}-\d{2})',
            r'_(\d+)\.md$'
        ]
        
        for pattern in version_patterns:
            match = re.search(pattern, filename, re.IGNORECASE)
            if match:
                return match.group(1)
        
        return None
    
    def _is_duplicate_content(self, analysis: FileAnalysis, all_analyses: List[FileAnalysis]) -> bool:
        """Check if file content is duplicate of another file."""
        if analysis.metadata.word_count < 20:  # Too small to reliably detect duplicates
            return False
        
        current_topics = set(analysis.metadata.key_topics)
        
        for other_analysis in all_analyses:
            if other_analysis.filename == analysis.filename:
                continue
            
            # Compare word counts (similar size)
            word_count_ratio = min(analysis.metadata.word_count, other_analysis.metadata.word_count) / \
                             max(analysis.metadata.word_count, other_analysis.metadata.word_count)
            
            if word_count_ratio < 0.8:  # Different sizes, likely not duplicates
                continue
            
            # Compare topics
            other_topics = set(other_analysis.metadata.key_topics)
            if current_topics and other_topics:
                topic_overlap = len(current_topics & other_topics) / len(current_topics | other_topics)
                if topic_overlap > 0.8:  # 80% topic overlap
                    return True
        
        return False
    
    def _is_temporary_file(self, filename: str) -> bool:
        """Check if filename indicates a temporary or draft file."""
        temp_indicators = [
            'temp', 'tmp', 'draft', 'backup', 'copy', 'old', 'bak',
            'test_', '_test', 'scratch', 'notes', 'wip'
        ]
        
        filename_lower = filename.lower()
        return any(indicator in filename_lower for indicator in temp_indicators)
    
    def _is_deprecated_content(self, analysis: FileAnalysis) -> bool:
        """Check if content appears to be deprecated."""
        deprecated_indicators = [
            'deprecated', 'obsolete', 'legacy', 'old_', 'archived',
            'removed', 'discontinued', 'superseded'
        ]
        
        filename_lower = analysis.filename.lower()
        
        # Check filename for deprecated indicators
        if any(indicator in filename_lower for indicator in deprecated_indicators):
            return True
        
        # Check topics for deprecated indicators
        topics_text = ' '.join(analysis.metadata.key_topics).lower()
        if any(indicator in topics_text for indicator in deprecated_indicators):
            return True
        
        return False
    
    def detect_version_conflicts(self, analyses: List[FileAnalysis]) -> Dict[str, List[FileAnalysis]]:
        """
        Detect version conflicts between files.
        
        Implements Requirement 7.2: Detect version conflicts and superseded files.
        
        Args:
            analyses: List of FileAnalysis objects
            
        Returns:
            Dictionary mapping conflict groups to lists of conflicting files
        """
        self.logger.info("Detecting version conflicts between files")
        
        conflicts = {}
        
        # Group files by similar base names
        base_name_groups = {}
        for analysis in analyses:
            base_name = self._extract_base_filename(analysis.filename)
            if base_name not in base_name_groups:
                base_name_groups[base_name] = []
            base_name_groups[base_name].append(analysis)
        
        # Check each group for conflicts
        for base_name, group in base_name_groups.items():
            if len(group) <= 1:
                continue
            
            # Look for version conflicts
            versioned_files = []
            completion_files = []
            similar_files = []
            
            for analysis in group:
                if self._extract_version_from_filename(analysis.filename):
                    versioned_files.append(analysis)
                elif 'complete' in analysis.filename.lower() or 'summary' in analysis.filename.lower():
                    completion_files.append(analysis)
                else:
                    similar_files.append(analysis)
            
            # Report version conflicts
            if len(versioned_files) > 1:
                conflict_key = f"version_conflict_{base_name}"
                conflicts[conflict_key] = versioned_files
                self.logger.warning(f"Version conflict detected for {base_name}: "
                                  f"{[f.filename for f in versioned_files]}")
            
            # Report completion file conflicts
            if len(completion_files) > 1:
                conflict_key = f"completion_conflict_{base_name}"
                conflicts[conflict_key] = completion_files
                self.logger.warning(f"Completion file conflict detected for {base_name}: "
                                  f"{[f.filename for f in completion_files]}")
            
            # Report similar file conflicts
            if len(similar_files) > 1:
                # Check if they're actually similar content
                if self._are_files_similar_content(similar_files):
                    conflict_key = f"content_conflict_{base_name}"
                    conflicts[conflict_key] = similar_files
                    self.logger.warning(f"Content conflict detected for {base_name}: "
                                      f"{[f.filename for f in similar_files]}")
        
        self.logger.info(f"Detected {len(conflicts)} version conflicts")
        return conflicts
    
    def create_freshness_indicators(self, analyses: List[FileAnalysis]) -> Dict[str, str]:
        """
        Create freshness indicators for content.
        
        Implements Requirement 7.4: Provide clear indicators about content freshness.
        
        Args:
            analyses: List of FileAnalysis objects
            
        Returns:
            Dictionary mapping filenames to freshness indicators
        """
        self.logger.info("Creating freshness indicators for content")
        
        freshness_indicators = {}
        current_time = datetime.now()
        
        for analysis in analyses:
            if not analysis.metadata.last_modified:
                freshness_indicators[analysis.filename] = " Unknown age"
                continue
            
            age_days = (current_time - analysis.metadata.last_modified).days
            
            # Create freshness indicator based on age and content type
            if age_days <= 7:
                indicator = " Fresh (updated within 1 week)"
            elif age_days <= 30:
                indicator = " Recent (updated within 1 month)"
            elif age_days <= 90:
                indicator = " Aging (updated within 3 months)"
            elif age_days <= 180:
                indicator = " Stale (updated within 6 months)"
            else:
                indicator = f" Old (last updated {age_days} days ago)"
            
            # Add content-specific context
            if analysis.content_type == ContentType.COMPLETION_SUMMARY:
                if age_days > 365:
                    indicator += " - Consider archiving"
                elif age_days > 180:
                    indicator += " - May need review"
            
            elif analysis.content_type == ContentType.SETUP_PROCEDURE:
                if age_days > 90:
                    indicator += " - Verify current accuracy"
                elif age_days > 30:
                    indicator += " - Check for updates"
            
            elif analysis.content_type == ContentType.TEST_REPORT:
                if age_days > 30:
                    indicator += " - Likely outdated"
                elif age_days > 7:
                    indicator += " - May be outdated"
            
            elif analysis.content_type == ContentType.FEATURE_GUIDE:
                if age_days > 180:
                    indicator += " - Verify feature status"
                elif age_days > 90:
                    indicator += " - Check for changes"
            
            freshness_indicators[analysis.filename] = indicator
        
        self.logger.info(f"Created freshness indicators for {len(freshness_indicators)} files")
        return freshness_indicators
    
    def flag_for_archival_or_removal(self, analyses: List[FileAnalysis]) -> Dict[str, Dict[str, List[FileAnalysis]]]:
        """
        Flag files for archival or removal with detailed recommendations.
        
        Implements Requirement 7.5: Flag files for archival or removal.
        
        Args:
            analyses: List of FileAnalysis objects
            
        Returns:
            Dictionary with archival and removal recommendations
        """
        self.logger.info("Flagging files for archival or removal")
        
        recommendations = {
            'archive': {
                'high_priority': [],
                'medium_priority': [],
                'low_priority': []
            },
            'remove': {
                'safe_to_remove': [],
                'review_required': [],
                'keep_but_consolidate': []
            }
        }
        
        for analysis in analyses:
            # Determine archival recommendations
            archive_priority = self._determine_archive_priority(analysis)
            if archive_priority:
                recommendations['archive'][archive_priority].append(analysis)
            
            # Determine removal recommendations
            removal_category = self._determine_removal_category(analysis)
            if removal_category:
                recommendations['remove'][removal_category].append(analysis)
        
        # Log summary
        total_archive = sum(len(files) for files in recommendations['archive'].values())
        total_remove = sum(len(files) for files in recommendations['remove'].values())
        
        self.logger.info(f"Archival recommendations: {total_archive} files")
        for priority, files in recommendations['archive'].items():
            if files:
                self.logger.info(f"  {priority}: {len(files)} files")
        
        self.logger.info(f"Removal recommendations: {total_remove} files")
        for category, files in recommendations['remove'].items():
            if files:
                self.logger.info(f"  {category}: {len(files)} files")
        
        return recommendations
    
    def _are_files_similar_content(self, files: List[FileAnalysis]) -> bool:
        """Check if files have similar content."""
        if len(files) < 2:
            return False
        
        # Compare the first file with all others
        base_file = files[0]
        base_topics = set(base_file.metadata.key_topics)
        
        for other_file in files[1:]:
            other_topics = set(other_file.metadata.key_topics)
            
            if base_topics and other_topics:
                overlap = len(base_topics & other_topics) / len(base_topics | other_topics)
                if overlap > 0.6:  # 60% topic overlap
                    return True
        
        return False
    
    def _determine_archive_priority(self, analysis: FileAnalysis) -> Optional[str]:
        """Determine archive priority for a file."""
        if not analysis.metadata.last_modified:
            return None
        
        age_days = (datetime.now() - analysis.metadata.last_modified).days
        
        # High priority archival
        if (analysis.content_type == ContentType.COMPLETION_SUMMARY and age_days > 365):
            return 'high_priority'
        
        if (analysis.content_type == ContentType.TEST_REPORT and age_days > 90):
            return 'high_priority'
        
        # Medium priority archival
        if (analysis.content_type == ContentType.SETUP_PROCEDURE and age_days > 180):
            return 'medium_priority'
        
        if (analysis.content_type == ContentType.FEATURE_GUIDE and age_days > 365):
            return 'medium_priority'
        
        # Low priority archival
        if (analysis.preservation_priority == Priority.LOW and age_days > 180):
            return 'low_priority'
        
        if self._is_deprecated_content(analysis):
            return 'medium_priority'
        
        return None
    
    def _determine_removal_category(self, analysis: FileAnalysis) -> Optional[str]:
        """Determine removal category for a file."""
        # Safe to remove
        if analysis.metadata.word_count < 10:
            return 'safe_to_remove'
        
        if self._is_temporary_file(analysis.filename):
            return 'safe_to_remove'
        
        if (analysis.content_type == ContentType.TEST_REPORT and 
            analysis.metadata.last_modified and
            (datetime.now() - analysis.metadata.last_modified).days > 90):
            return 'safe_to_remove'
        
        # Review required
        if self._is_duplicate_content(analysis, []):  # Would need full list in real implementation
            return 'review_required'
        
        # Keep but consolidate
        if (analysis.content_type == ContentType.COMPLETION_SUMMARY and
            len(analysis.metadata.key_topics) > 0):
            return 'keep_but_consolidate'
        
        return None